Writing to /root/.config/pip/pip.conf
Writing to /root/.config/pip/pip.conf
Writing to /root/.config/pip/pip.conf
Looking in indexes: http://cache-service.nginx-pypi-cache.svc.cluster.local/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Collecting kubernetes
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ca/ec/65f7d563aa4a62dd58777e8f6aa882f15db53b14eb29aba0c28a20f7eb26/kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)
Requirement already satisfied: certifi>=14.05.14 in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (2025.11.12)
Requirement already satisfied: six>=1.9.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (1.17.0)
Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (2.9.0.post0)
Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (6.0.3)
Collecting google-auth>=1.0.1 (from kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c6/97/451d55e05487a5cd6279a01a7e34921858b16f7dc8aa38a2c684743cd2b3/google_auth-2.45.0-py2.py3-none-any.whl (233 kB)
Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/34/db/b10e48aa8fff7407e67470363eac595018441cf32d5e1001567a7aeba5d2/websocket_client-1.9.0-py3-none-any.whl (82 kB)
Requirement already satisfied: requests in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (2.32.5)
Collecting requests-oauthlib (from kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)
Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c8/19/4ec628951a74043532ca2cf5d97b7b14863931476d117c471e8e2b1eb39f/urllib3-2.3.0-py3-none-any.whl (128 kB)
Collecting durationpy>=0.7 (from kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/b0/0d/9feae160378a3553fa9a339b0e9c1a048e147a4127210e286ef18b730f03/durationpy-0.10-py3-none-any.whl (3.9 kB)
Collecting cachetools<7.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/2c/fc/1d7b80d0eb7b714984ce40efc78859c022cd930e402f599d8ca9e39c78a4/cachetools-6.2.4-py3-none-any.whl (11 kB)
Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)
Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl (34 kB)
Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl (83 kB)
Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->kubernetes) (3.4.4)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->kubernetes) (3.11)
Collecting oauthlib>=3.0.0 (from requests-oauthlib->kubernetes)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/be/9c/92789c596b8df838baa98fa71844d84283302f7604ed565dafe5a6b5041a/oauthlib-3.3.1-py3-none-any.whl (160 kB)
Installing collected packages: durationpy, websocket-client, urllib3, pyasn1, oauthlib, cachetools, rsa, pyasn1-modules, requests-oauthlib, google-auth, kubernetes
  Attempting uninstall: urllib3
    Found existing installation: urllib3 2.5.0
    Uninstalling urllib3-2.5.0:
      Successfully uninstalled urllib3-2.5.0

Successfully installed cachetools-6.2.4 durationpy-0.10 google-auth-2.45.0 kubernetes-34.1.0 oauthlib-3.3.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0 rsa-4.9.1 urllib3-2.3.0 websocket-client-1.9.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  732k  100  732k    0     0  1522k      0 --:--:-- --:--:-- --:--:-- 1525k
performance
vm.swappiness = 0
kernel.numa_balancing = 0
kernel.sched_migration_cost_ns = 50000
Running test case test/nightly/ascend/performance/test_ascend_deepseek_v3.2_w8a8_1p1d_32p_in64k_out1k_30ms.py
The nic name matched is enp23s0f3
The nic name matched is enp23s0f3
Nic name: enp23s0f3
Init 192.168.0.77 cls.role=None!
[CI Test Method] Test_DeepSeek_V3_2_W8A8_1P1D.test_throughput
launch_node start ......
query_configmap successfully!
monitor configmap.data={'sglang-multi-debug-sglang-decode-0': '192.168.0.184', 'sglang-multi-debug-sglang-decode-1': '192.168.0.77', 'sglang-multi-debug-sglang-prefill-0': '192.168.0.81', 'sglang-multi-debug-sglang-prefill-1': '192.168.0.60', 'sglang-multi-debug-sglang-router-0': '192.168.0.77'}
ENV_VAR ASCEND_MF_STORE_URL=tcp://192.168.0.81:24666
launch decode node dist_init_addr='192.168.0.184:5000'
ENV_VAR SGLANG_SET_CPU_AFFINITY=1
ENV_VAR PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
ENV_VAR STREAMS_PER_DEVICE=32
ENV_VAR SGLANG_NPU_USE_MULTI_STREAM=1
ENV_VAR SGLANG_NPU_USE_MLAPO=1
ENV_VAR HCCL_OP_EXPANSION_MODE=AIV
ENV_VAR SGLANG_SCHEDULER_SKIP_ALL_GATHER=1
ENV_VAR TASK_QUEUE_ENABLE=0
ENV_VAR SGLANG_ENABLE_OVERLAP_PLAN_STREAM=1
ENV_VAR SGLANG_ENABLE_SPEC_V2=1
ENV_VAR HCCL_SOCKET_IFNAME=enp23s0f3
ENV_VAR GLOO_SOCKET_IFNAME=enp23s0f3
ENV_VAR HCCL_BUFFSIZE=400
ENV_VAR SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=8
No node-rank specified and all decode node will form a single instance.
Starting node, node_ip='192.168.0.77' service_args=['--trust-remote-code', '--attention-backend', 'ascend', '--device', 'npu', '--disaggregation-transfer-backend', 'ascend', '--nnodes', '2', '--disaggregation-mode', 'decode', '--tp', 32, '--dp', 8, '--ep', 32, '--moe-dense-tp-size', 1, '--enable-dp-attention', '--enable-dp-lm-head', '--watchdog-timeout', 9000, '--mem-fraction-static', 0.79, '--disable-radix-cache', '--chunked-prefill-size', -1, '--max-prefill-tokens', 68000, '--max-running-requests', '32', '--cuda-graph-max-bs', '4', '--moe-a2a-backend', 'deepep', '--deepep-mode', 'low_latency', '--quantization', 'modelslim', '--speculative-algorithm', 'NEXTN', '--speculative-num-steps', 3, '--speculative-eagle-topk', 1, '--speculative-num-draft-tokens', 4, '--prefill-round-robin-balance', '--load-balance-method', 'decode_round_robin', '--dist-init-addr', '192.168.0.184:5000', '--node-rank', 1]
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8 --trust-remote-code --attention-backend ascend --device npu --disaggregation-transfer-backend ascend --nnodes 2 --disaggregation-mode decode --tp 32 --dp 8 --ep 32 --moe-dense-tp-size 1 --enable-dp-attention --enable-dp-lm-head --watchdog-timeout 9000 --mem-fraction-static 0.79 --disable-radix-cache --chunked-prefill-size -1 --max-prefill-tokens 68000 --max-running-requests 32 --cuda-graph-max-bs 4 --moe-a2a-backend deepep --deepep-mode low_latency --quantization modelslim --speculative-algorithm NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --prefill-round-robin-balance --load-balance-method decode_round_robin --dist-init-addr 192.168.0.184:5000 --node-rank 1 --device npu --host 192.168.0.77 --port 8000
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-31 09:49:50] WARNING model_config.py:814: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:49:50] WARNING server_args.py:1776: DP attention is enabled. The chunked prefill size is adjusted to -1 to avoid MoE kernel issues. 
[2025-12-31 09:49:50] WARNING server_args.py:1829: DeepEP MoE is enabled. The expert parallel size is adjusted to be the same as the tensor parallel size[32].
[2025-12-31 09:49:50] WARNING server_args.py:1971: Beta spec is enabled for eagle speculative decoding and overlap schedule is turned on.
[2025-12-31 09:49:50] WARNING server_args.py:2212: KV cache is forced as chunk cache for decode server
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-31 09:49:51] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', tokenizer_path='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='192.168.0.77', port=8000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.79, max_running_requests=32, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=-1, enable_dynamic_chunking=False, max_prefill_tokens=68000, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=32, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=21309810, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=9000.0, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=8, load_balance_method='decode_round_robin', load_watch_interval=0.1, prefill_round_robin_balance=True, dist_init_addr='192.168.0.184:5000', nnodes=2, node_rank=1, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm='EAGLE', speculative_draft_model_path='/root/.cache/modelscope/hub/models/DeepSeek-V3.2-Exp-W8A8', speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=3, speculative_eagle_topk=1, speculative_num_draft_tokens=4, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, speculative_suffix_max_tree_depth=24, speculative_suffix_max_cached_requests=10000, speculative_suffix_max_spec_factor=1.0, speculative_suffix_min_token_prob=0.1, enable_mtp=False, ep_size=32, moe_a2a_backend='deepep', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='low_latency', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=1, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=4, cuda_graph_bs=[1, 2, 3, 4], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=True, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='decode', disaggregation_transfer_backend='ascend', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-31 09:50:16 DP4 TP18 EP18] Process 1423 gpu_id 2 is running on CPUs: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-12-31 09:50:16 DP4 TP19 EP19] Process 1424 gpu_id 3 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
[2025-12-31 09:50:16 DP6 TP27 EP27] Process 1810 gpu_id 11 is running on CPUs: [220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-12-31 09:50:16 DP6 TP27 EP27] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:16 DP4 TP18 EP18] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:16 DP4 TP19 EP19] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:16 DP5 TP22 EP22] Process 1805 gpu_id 6 is running on CPUs: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
[2025-12-31 09:50:16 DP4 TP16 EP16] Process 1421 gpu_id 0 is running on CPUs: [0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
[2025-12-31 09:50:16 DP5 TP20 EP20] Process 1425 gpu_id 4 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
[2025-12-31 09:50:16 DP5 TP22 EP22] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:16 DP4 TP16 EP16] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:16 DP5 TP20 EP20] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP7 TP29 EP29] Process 1812 gpu_id 13 is running on CPUs: [260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279]
[2025-12-31 09:50:17 DP7 TP30 EP30] Process 1813 gpu_id 14 is running on CPUs: [280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]
[2025-12-31 09:50:17 DP5 TP23 EP23] Process 1806 gpu_id 7 is running on CPUs: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-12-31 09:50:17 DP6 TP26 EP26] Process 1809 gpu_id 10 is running on CPUs: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
[2025-12-31 09:50:17 DP7 TP29 EP29] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP7 TP30 EP30] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP5 TP23 EP23] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP6 TP26 EP26] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP6 TP25 EP25] Process 1808 gpu_id 9 is running on CPUs: [180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
[2025-12-31 09:50:17 DP6 TP25 EP25] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP5 TP21 EP21] Process 1426 gpu_id 5 is running on CPUs: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
[2025-12-31 09:50:17 DP5 TP21 EP21] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP4 TP17 EP17] Process 1422 gpu_id 1 is running on CPUs: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
[2025-12-31 09:50:17 DP7 TP28 EP28] Process 1811 gpu_id 12 is running on CPUs: [240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259]
[2025-12-31 09:50:17 DP7 TP31 EP31] Process 1814 gpu_id 15 is running on CPUs: [300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319]
[2025-12-31 09:50:17 DP6 TP24 EP24] Process 1807 gpu_id 8 is running on CPUs: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]
[2025-12-31 09:50:17 DP7 TP28 EP28] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP4 TP17 EP17] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP6 TP24 EP24] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:17 DP7 TP31 EP31] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2025-12-31 09:50:18 DP5 TP20 EP20] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP5 TP21 EP21] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP5 TP20 EP20] Init torch distributed begin.
[2025-12-31 09:50:18 DP6 TP25 EP25] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP5 TP21 EP21] Init torch distributed begin.
[2025-12-31 09:50:18 DP6 TP24 EP24] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP5 TP22 EP22] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP4 TP17 EP17] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP5 TP23 EP23] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP4 TP16 EP16] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP6 TP25 EP25] Init torch distributed begin.
[2025-12-31 09:50:18 DP6 TP27 EP27] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP6 TP24 EP24] Init torch distributed begin.
[2025-12-31 09:50:18 DP6 TP26 EP26] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP5 TP22 EP22] Init torch distributed begin.
[2025-12-31 09:50:18 DP4 TP18 EP18] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP4 TP17 EP17] Init torch distributed begin.
[2025-12-31 09:50:18 DP5 TP23 EP23] Init torch distributed begin.
[2025-12-31 09:50:18 DP4 TP19 EP19] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP7 TP28 EP28] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP7 TP29 EP29] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP4 TP16 EP16] Init torch distributed begin.
[2025-12-31 09:50:18 DP6 TP27 EP27] Init torch distributed begin.
[2025-12-31 09:50:18 DP6 TP26 EP26] Init torch distributed begin.
[2025-12-31 09:50:18 DP4 TP18 EP18] Init torch distributed begin.
[2025-12-31 09:50:18 DP7 TP30 EP30] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP7 TP31 EP31] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:50:18 DP7 TP28 EP28] Init torch distributed begin.
[2025-12-31 09:50:18 DP4 TP19 EP19] Init torch distributed begin.
[2025-12-31 09:50:18 DP7 TP29 EP29] Init torch distributed begin.
[2025-12-31 09:50:18 DP7 TP30 EP30] Init torch distributed begin.
[2025-12-31 09:50:18 DP7 TP31 EP31] Init torch distributed begin.
[Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-12-31 09:50:21 DP7 TP31 EP31] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP7 TP30 EP30] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP7 TP28 EP28] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP7 TP29 EP29] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP6 TP27 EP27] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP6 TP26 EP26] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP6 TP25 EP25] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP6 TP24 EP24] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP5 TP23 EP23] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP5 TP22 EP22] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP5 TP21 EP21] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP5 TP20 EP20] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP4 TP18 EP18] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP4 TP19 EP19] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP4 TP17 EP17] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:50:21 DP4 TP16 EP16] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-31 09:50:22 DP4 TP16 EP16] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP5 TP21 EP21] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP4 TP17 EP17] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP6 TP24 EP24] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP5 TP22 EP22] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP5 TP23 EP23] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP6 TP25 EP25] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP6 TP27 EP27] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP5 TP20 EP20] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP4 TP19 EP19] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP7 TP28 EP28] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP6 TP26 EP26] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP4 TP18 EP18] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP7 TP29 EP29] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP7 TP31 EP31] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP7 TP30 EP30] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-31 09:50:22 DP6 TP25 EP25] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP5 TP23 EP23] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP4 TP19 EP19] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP5 TP21 EP21] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP4 TP16 EP16] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP5 TP20 EP20] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP4 TP17 EP17] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP6 TP24 EP24] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP5 TP22 EP22] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP4 TP18 EP18] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP6 TP27 EP27] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP7 TP28 EP28] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP7 TP29 EP29] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP6 TP26 EP26] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP7 TP31 EP31] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-31 09:50:22 DP7 TP30 EP30] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-31 09:50:24 DP5 TP20 EP20] Load weight begin. avail mem=60.85 GB
[2025-12-31 09:50:24 DP5 TP21 EP21] Load weight begin. avail mem=61.08 GB
[2025-12-31 09:50:24 DP5 TP22 EP22] Load weight begin. avail mem=60.85 GB
[2025-12-31 09:50:24 DP6 TP25 EP25] Load weight begin. avail mem=61.09 GB
[2025-12-31 09:50:24 DP5 TP23 EP23] Load weight begin. avail mem=61.08 GB
[2025-12-31 09:50:24 DP7 TP28 EP28] Load weight begin. avail mem=60.85 GB
[2025-12-31 09:50:24 DP6 TP24 EP24] Load weight begin. avail mem=60.84 GB
[2025-12-31 09:50:24 DP7 TP31 EP31] Load weight begin. avail mem=61.08 GB
[2025-12-31 09:50:24 DP7 TP30 EP30] Load weight begin. avail mem=60.85 GB
[2025-12-31 09:50:24 DP6 TP26 EP26] Load weight begin. avail mem=60.85 GB
[2025-12-31 09:50:24 DP4 TP17 EP17] Load weight begin. avail mem=61.08 GB
[2025-12-31 09:50:24 DP7 TP29 EP29] Load weight begin. avail mem=61.08 GB
[2025-12-31 09:50:24 DP6 TP27 EP27] Load weight begin. avail mem=61.08 GB
[2025-12-31 09:50:24 DP4 TP16 EP16] Load weight begin. avail mem=60.78 GB
[2025-12-31 09:50:24 DP4 TP18 EP18] Load weight begin. avail mem=60.86 GB
[2025-12-31 09:50:24 DP4 TP19 EP19] Load weight begin. avail mem=61.08 GB
[2025-12-31 09:55:59 DP7 TP28 EP28] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=28.93 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP6 TP25 EP25] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=29.17 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP5 TP22 EP22] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=28.94 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP5 TP20 EP20] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=28.94 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP4 TP17 EP17] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=29.17 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP4 TP18 EP18] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=28.94 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP7 TP29 EP29] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=29.17 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP4 TP19 EP19] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=29.16 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP7 TP30 EP30] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=28.94 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP5 TP23 EP23] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=29.16 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP6 TP26 EP26] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=28.94 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP6 TP27 EP27] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=29.16 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP5 TP21 EP21] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=29.16 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP4 TP16 EP16] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=28.86 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP7 TP31 EP31] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=29.16 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP6 TP24 EP24] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=28.93 GB, mem usage=31.91 GB.
[2025-12-31 09:55:59 DP7 TP31 EP31] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP7 TP30 EP30] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP7 TP29 EP29] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP7 TP28 EP28] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP6 TP27 EP27] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP6 TP26 EP26] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP6 TP25 EP25] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP6 TP24 EP24] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP5 TP23 EP23] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP5 TP22 EP22] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP5 TP21 EP21] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP5 TP20 EP20] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP4 TP19 EP19] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP4 TP18 EP18] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP4 TP17 EP17] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP4 TP16 EP16] The available memory for KV cache is 16.06 GB.
[2025-12-31 09:55:59 DP4 TP18 EP18] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP7 TP28 EP28] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP6 TP26 EP26] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP7 TP30 EP30] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP4 TP17 EP17] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP5 TP20 EP20] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP6 TP25 EP25] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP4 TP19 EP19] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP4 TP16 EP16] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP7 TP29 EP29] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP7 TP30 EP30] Memory pool end. avail mem=11.24 GB
[2025-12-31 09:55:59 DP4 TP18 EP18] Memory pool end. avail mem=11.25 GB
[2025-12-31 09:55:59 DP7 TP28 EP28] Memory pool end. avail mem=11.24 GB
[2025-12-31 09:55:59 DP4 TP17 EP17] Memory pool end. avail mem=11.47 GB
[2025-12-31 09:55:59 DP5 TP23 EP23] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP5 TP21 EP21] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP6 TP26 EP26] Memory pool end. avail mem=11.24 GB
[2025-12-31 09:55:59 DP5 TP20 EP20] Memory pool end. avail mem=11.24 GB
[2025-12-31 09:55:59 DP6 TP24 EP24] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP4 TP19 EP19] Memory pool end. avail mem=11.47 GB
[2025-12-31 09:55:59 DP4 TP16 EP16] Memory pool end. avail mem=11.17 GB
[2025-12-31 09:55:59 DP7 TP29 EP29] Memory pool end. avail mem=11.47 GB
[2025-12-31 09:55:59 DP6 TP27 EP27] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP7 TP31 EP31] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP5 TP23 EP23] Memory pool end. avail mem=11.47 GB
[2025-12-31 09:55:59 DP5 TP21 EP21] Memory pool end. avail mem=11.47 GB
[2025-12-31 09:55:59 DP6 TP25 EP25] Memory pool end. avail mem=11.48 GB
[2025-12-31 09:55:59 DP6 TP24 EP24] Memory pool end. avail mem=11.23 GB
[2025-12-31 09:55:59 DP5 TP22 EP22] KV Cache is allocated. #tokens: 220416, KV size: 17.64 GB
[2025-12-31 09:55:59 DP6 TP27 EP27] Memory pool end. avail mem=11.47 GB
[2025-12-31 09:55:59 DP7 TP31 EP31] Memory pool end. avail mem=11.47 GB
[2025-12-31 09:55:59 DP5 TP22 EP22] Memory pool end. avail mem=11.25 GB
[2025-12-31 09:55:59 DP6 TP26 EP26] Capture cuda graph begin. This can take up to several minutes. avail mem=11.24 GB
[2025-12-31 09:55:59 DP6 TP25 EP25] Capture cuda graph begin. This can take up to several minutes. avail mem=11.48 GB
[2025-12-31 09:55:59 DP5 TP23 EP23] Capture cuda graph begin. This can take up to several minutes. avail mem=11.47 GB
[2025-12-31 09:55:59 DP5 TP22 EP22] Capture cuda graph begin. This can take up to several minutes. avail mem=11.24 GB
[2025-12-31 09:55:59 DP6 TP24 EP24] Capture cuda graph begin. This can take up to several minutes. avail mem=11.23 GB
[2025-12-31 09:55:59 DP7 TP28 EP28] Capture cuda graph begin. This can take up to several minutes. avail mem=11.24 GB
[2025-12-31 09:55:59 DP5 TP20 EP20] Capture cuda graph begin. This can take up to several minutes. avail mem=11.24 GB
[2025-12-31 09:55:59 DP7 TP29 EP29] Capture cuda graph begin. This can take up to several minutes. avail mem=11.47 GB
[2025-12-31 09:55:59 DP5 TP21 EP21] Capture cuda graph begin. This can take up to several minutes. avail mem=11.47 GB
[2025-12-31 09:55:59 DP4 TP16 EP16] Capture cuda graph begin. This can take up to several minutes. avail mem=11.17 GB
[2025-12-31 09:55:59 DP4 TP17 EP17] Capture cuda graph begin. This can take up to several minutes. avail mem=11.47 GB
[2025-12-31 09:55:59 DP7 TP30 EP30] Capture cuda graph begin. This can take up to several minutes. avail mem=11.24 GB
[2025-12-31 09:55:59 DP7 TP31 EP31] Capture cuda graph begin. This can take up to several minutes. avail mem=11.47 GB
[2025-12-31 09:55:59 DP4 TP18 EP18] Capture cuda graph begin. This can take up to several minutes. avail mem=11.24 GB
[2025-12-31 09:55:59 DP4 TP19 EP19] Capture cuda graph begin. This can take up to several minutes. avail mem=11.46 GB
[2025-12-31 09:55:59 DP6 TP27 EP27] Capture cuda graph begin. This can take up to several minutes. avail mem=11.47 GB
[rank25]:[W1231 09:56:24.998856543 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank24]:[W1231 09:56:24.998864213 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank26]:[W1231 09:56:24.998871503 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank30]:[W1231 09:56:24.998872653 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank31]:[W1231 09:56:24.998872953 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank28]:[W1231 09:56:24.998876593 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank19]:[W1231 09:56:24.998881013 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank29]:[W1231 09:56:24.998915324 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank27]:[W1231 09:56:24.999108995 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank18]:[W1231 09:56:24.999838198 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank23]:[W1231 09:56:24.000336510 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank16]:[W1231 09:56:24.000347000 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank22]:[W1231 09:56:24.000664282 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank20]:[W1231 09:56:24.001355345 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank17]:[W1231 09:56:24.001422536 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank21]:[W1231 09:56:24.001566356 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[2025-12-31 09:56:25 DP5 TP21 EP21] Capture cuda graph end. Time elapsed: 25.83 s. mem usage=2.00 GB. avail mem=9.47 GB.
[2025-12-31 09:56:25 DP5 TP20 EP20] Capture cuda graph end. Time elapsed: 25.86 s. mem usage=2.00 GB. avail mem=9.24 GB.
[2025-12-31 09:56:25 DP5 TP22 EP22] Capture cuda graph end. Time elapsed: 25.91 s. mem usage=2.00 GB. avail mem=9.25 GB.
[2025-12-31 09:56:25 DP6 TP25 EP25] Capture cuda graph end. Time elapsed: 25.92 s. mem usage=2.00 GB. avail mem=9.48 GB.
[2025-12-31 09:56:25 DP6 TP24 EP24] Capture cuda graph end. Time elapsed: 25.93 s. mem usage=2.00 GB. avail mem=9.23 GB.
[2025-12-31 09:56:25 DP5 TP23 EP23] Capture cuda graph end. Time elapsed: 25.94 s. mem usage=2.00 GB. avail mem=9.47 GB.
[2025-12-31 09:56:25 DP7 TP28 EP28] Capture cuda graph end. Time elapsed: 25.95 s. mem usage=2.00 GB. avail mem=9.24 GB.
[2025-12-31 09:56:25 DP7 TP31 EP31] Capture cuda graph end. Time elapsed: 25.96 s. mem usage=2.00 GB. avail mem=9.47 GB.
[2025-12-31 09:56:25 DP7 TP30 EP30] Capture cuda graph end. Time elapsed: 25.96 s. mem usage=2.00 GB. avail mem=9.24 GB.
[2025-12-31 09:56:25 DP6 TP26 EP26] Capture cuda graph end. Time elapsed: 25.97 s. mem usage=2.00 GB. avail mem=9.24 GB.
[2025-12-31 09:56:25 DP4 TP17 EP17] Capture cuda graph end. Time elapsed: 26.00 s. mem usage=2.00 GB. avail mem=9.47 GB.
[2025-12-31 09:56:25 DP6 TP27 EP27] Capture cuda graph end. Time elapsed: 26.01 s. mem usage=2.00 GB. avail mem=9.47 GB.
[2025-12-31 09:56:25 DP4 TP16 EP16] Capture cuda graph end. Time elapsed: 26.02 s. mem usage=2.00 GB. avail mem=9.17 GB.
[2025-12-31 09:56:25 DP7 TP29 EP29] Capture cuda graph end. Time elapsed: 26.02 s. mem usage=2.00 GB. avail mem=9.47 GB.
[2025-12-31 09:56:26 DP4 TP19 EP19] Capture cuda graph end. Time elapsed: 26.05 s. mem usage=2.00 GB. avail mem=9.47 GB.
[2025-12-31 09:56:26 DP4 TP18 EP18] Capture cuda graph end. Time elapsed: 26.07 s. mem usage=2.00 GB. avail mem=9.25 GB.
[2025-12-31 09:56:26 DP6 TP24 EP24] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP6 TP25 EP25] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP5 TP21 EP21] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP5 TP20 EP20] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP7 TP29 EP29] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP7 TP28 EP28] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP4 TP16 EP16] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP4 TP17 EP17] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP5 TP20 EP20] Init torch distributed begin.
[2025-12-31 09:56:26 DP5 TP21 EP21] Init torch distributed begin.
[2025-12-31 09:56:26 DP6 TP24 EP24] Init torch distributed begin.
[2025-12-31 09:56:26 DP6 TP25 EP25] Init torch distributed begin.
[2025-12-31 09:56:26 DP7 TP28 EP28] Init torch distributed begin.
[2025-12-31 09:56:26 DP7 TP29 EP29] Init torch distributed begin.
[2025-12-31 09:56:26 DP4 TP17 EP17] Init torch distributed begin.
[2025-12-31 09:56:26 DP4 TP16 EP16] Init torch distributed begin.
[2025-12-31 09:56:26 DP5 TP23 EP23] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP5 TP23 EP23] Init torch distributed begin.
[2025-12-31 09:56:26 DP6 TP27 EP27] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP6 TP27 EP27] Init torch distributed begin.
[2025-12-31 09:56:26 DP7 TP31 EP31] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP7 TP31 EP31] Init torch distributed begin.
[2025-12-31 09:56:26 DP4 TP19 EP19] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP4 TP19 EP19] Init torch distributed begin.
[2025-12-31 09:56:26 DP5 TP22 EP22] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP5 TP22 EP22] Init torch distributed begin.
[2025-12-31 09:56:26 DP4 TP18 EP18] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP6 TP26 EP26] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP7 TP30 EP30] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-31 09:56:26 DP4 TP18 EP18] Init torch distributed begin.
[2025-12-31 09:56:26 DP6 TP26 EP26] Init torch distributed begin.
[2025-12-31 09:56:26 DP7 TP30 EP30] Init torch distributed begin.
[2025-12-31 09:56:26 DP7 TP30 EP30] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP7 TP31 EP31] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP7 TP28 EP28] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP7 TP29 EP29] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP6 TP26 EP26] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP6 TP25 EP25] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP6 TP27 EP27] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP6 TP24 EP24] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP5 TP22 EP22] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP5 TP23 EP23] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP5 TP20 EP20] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP5 TP21 EP21] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP4 TP16 EP16] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP4 TP19 EP19] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP4 TP18 EP18] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP4 TP17 EP17] Init torch distributed ends. mem usage=0.00 GB
[2025-12-31 09:56:26 DP7 TP30 EP30] Load weight begin. avail mem=9.24 GB
[2025-12-31 09:56:26 DP7 TP28 EP28] Load weight begin. avail mem=9.24 GB
[2025-12-31 09:56:26 DP7 TP31 EP31] Load weight begin. avail mem=9.47 GB
[2025-12-31 09:56:26 DP6 TP26 EP26] Load weight begin. avail mem=9.24 GB
[2025-12-31 09:56:26 DP7 TP29 EP29] Load weight begin. avail mem=9.47 GB
[2025-12-31 09:56:26 DP5 TP20 EP20] Load weight begin. avail mem=9.24 GB
[2025-12-31 09:56:26 DP5 TP22 EP22] Load weight begin. avail mem=9.25 GB
[2025-12-31 09:56:26 DP6 TP24 EP24] Load weight begin. avail mem=9.23 GB
[2025-12-31 09:56:26 DP6 TP25 EP25] Load weight begin. avail mem=9.48 GB
[2025-12-31 09:56:26 DP5 TP23 EP23] Load weight begin. avail mem=9.47 GB
[2025-12-31 09:56:26 DP5 TP21 EP21] Load weight begin. avail mem=9.47 GB
[2025-12-31 09:56:26 DP6 TP27 EP27] Load weight begin. avail mem=9.47 GB
[2025-12-31 09:56:26 DP4 TP16 EP16] Load weight begin. avail mem=9.17 GB
[2025-12-31 09:56:26 DP4 TP18 EP18] Load weight begin. avail mem=9.25 GB
[2025-12-31 09:56:26 DP4 TP17 EP17] Load weight begin. avail mem=9.47 GB
[2025-12-31 09:56:26 DP4 TP19 EP19] Load weight begin. avail mem=9.47 GB
[2025-12-31 09:59:17 DP7 TP29 EP29] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.70 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP6 TP25 EP25] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.71 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP4 TP17 EP17] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.70 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP4 TP19 EP19] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.69 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP5 TP23 EP23] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.69 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP6 TP27 EP27] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.70 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP7 TP31 EP31] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.69 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP5 TP20 EP20] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.47 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP5 TP22 EP22] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.47 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP5 TP21 EP21] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.69 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP7 TP28 EP28] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.47 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP4 TP18 EP18] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.47 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP7 TP30 EP30] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.47 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP6 TP24 EP24] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.46 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP6 TP26 EP26] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.47 GB, mem usage=2.77 GB.
[2025-12-31 09:59:17 DP4 TP16 EP16] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.40 GB, mem usage=2.77 GB.
[2025-12-31 09:59:18 DP7 TP31 EP31] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP7 TP30 EP30] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP7 TP29 EP29] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP7 TP28 EP28] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP6 TP27 EP27] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP6 TP26 EP26] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP6 TP25 EP25] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP6 TP24 EP24] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP5 TP23 EP23] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP5 TP22 EP22] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP5 TP21 EP21] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP5 TP20 EP20] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP4 TP19 EP19] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP4 TP18 EP18] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP4 TP17 EP17] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP4 TP16 EP16] The available memory for KV cache is 4.46 GB.
[2025-12-31 09:59:18 DP7 TP31 EP31] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP7 TP29 EP29] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP7 TP30 EP30] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP6 TP27 EP27] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP6 TP25 EP25] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP6 TP26 EP26] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP7 TP28 EP28] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP5 TP21 EP21] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP5 TP20 EP20] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP4 TP19 EP19] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP5 TP23 EP23] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP6 TP24 EP24] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP4 TP17 EP17] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP4 TP16 EP16] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP7 TP31 EP31] Memory pool end. avail mem=6.42 GB
[2025-12-31 09:59:18 DP5 TP22 EP22] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP7 TP30 EP30] Memory pool end. avail mem=6.20 GB
[2025-12-31 09:59:18 DP4 TP18 EP18] KV Cache is allocated. #tokens: 220416, KV size: 0.29 GB
[2025-12-31 09:59:18 DP6 TP25 EP25] Memory pool end. avail mem=6.43 GB
[2025-12-31 09:59:18 DP7 TP28 EP28] Memory pool end. avail mem=6.19 GB
[2025-12-31 09:59:18 DP6 TP26 EP26] Memory pool end. avail mem=6.20 GB
[2025-12-31 09:59:18 DP7 TP29 EP29] Memory pool end. avail mem=6.42 GB
[2025-12-31 09:59:18 DP6 TP27 EP27] Memory pool end. avail mem=6.42 GB
[2025-12-31 09:59:18 DP4 TP19 EP19] Memory pool end. avail mem=6.42 GB
[2025-12-31 09:59:18 DP5 TP23 EP23] Memory pool end. avail mem=6.42 GB
[2025-12-31 09:59:18 DP6 TP24 EP24] Memory pool end. avail mem=6.19 GB
[2025-12-31 09:59:18 DP5 TP20 EP20] Memory pool end. avail mem=6.20 GB
[2025-12-31 09:59:18 DP5 TP21 EP21] Memory pool end. avail mem=6.42 GB
[2025-12-31 09:59:18 DP4 TP17 EP17] Memory pool end. avail mem=6.42 GB
[2025-12-31 09:59:18 DP5 TP22 EP22] Memory pool end. avail mem=6.20 GB
[2025-12-31 09:59:18 DP4 TP16 EP16] Memory pool end. avail mem=6.12 GB
[2025-12-31 09:59:18 DP4 TP18 EP18] Memory pool end. avail mem=6.20 GB
[2025-12-31 09:59:19 DP6 TP27 EP27] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.53 GB
[2025-12-31 09:59:19 DP5 TP23 EP23] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.53 GB
[2025-12-31 09:59:19 DP4 TP19 EP19] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.52 GB
[2025-12-31 09:59:19 DP5 TP21 EP21] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.53 GB
[2025-12-31 09:59:19 DP5 TP20 EP20] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.30 GB
[2025-12-31 09:59:19 DP6 TP25 EP25] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.54 GB
[2025-12-31 09:59:19 DP7 TP28 EP28] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.30 GB
[2025-12-31 09:59:19 DP7 TP29 EP29] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.53 GB
[2025-12-31 09:59:19 DP5 TP22 EP22] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.30 GB
[2025-12-31 09:59:19 DP4 TP16 EP16] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.23 GB
[2025-12-31 09:59:19 DP7 TP31 EP31] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.53 GB
[2025-12-31 09:59:19 DP4 TP18 EP18] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.31 GB
[2025-12-31 09:59:19 DP6 TP26 EP26] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.30 GB
[2025-12-31 09:59:19 DP7 TP30 EP30] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.30 GB
[2025-12-31 09:59:19 DP6 TP24 EP24] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.29 GB
[2025-12-31 09:59:19 DP4 TP17 EP17] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.53 GB
[rank22]:[W1231 09:59:21.992632278 compiler_depend.ts:198] Warning: Driver Version: "j" is invalid or not supported yet. (function operator())
[rank23]:[W1231 09:59:21.992645698 compiler_depend.ts:198] Warning: Driver Version: "j" is invalid or not supported yet. (function operator())
[rank21]:[W1231 09:59:21.992646968 compiler_depend.ts:198] Warning: Driver Version: "@" is invalid or not supported yet. (function operator())
[rank20]:[W1231 09:59:21.992647138 compiler_depend.ts:198] Warning: Driver Version: "U" is invalid or not supported yet. (function operator())
[rank18]:[W1231 09:59:21.992646288 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank19]:[W1231 09:59:21.992646338 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank17]:[W1231 09:59:21.003658360 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank16]:[W1231 09:59:21.004041872 compiler_depend.ts:198] Warning: Driver Version: "O" is invalid or not supported yet. (function operator())
[rank24]:[W1231 09:59:21.013138195 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank25]:[W1231 09:59:21.013192825 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank27]:[W1231 09:59:21.026918201 compiler_depend.ts:198] Warning: Driver Version: "j" is invalid or not supported yet. (function operator())
[rank26]:[W1231 09:59:21.027120042 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank29]:[W1231 09:59:21.052543983 compiler_depend.ts:198] Warning: Driver Version: "?" is invalid or not supported yet. (function operator())
[rank28]:[W1231 09:59:21.053180556 compiler_depend.ts:198] Warning: Driver Version: "j" is invalid or not supported yet. (function operator())
[rank30]:[W1231 09:59:21.060555291 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank31]:[W1231 09:59:21.063721306 compiler_depend.ts:198] Warning: Driver Version: "" is invalid or not supported yet. (function operator())
[rank23]:[W1231 09:59:21.287294819 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank22]:[W1231 09:59:21.287549691 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank20]:[W1231 09:59:21.292665955 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank21]:[W1231 09:59:21.292666125 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank19]:[W1231 09:59:21.307421325 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank18]:[W1231 09:59:21.307421555 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank16]:[W1231 09:59:21.317458403 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank17]:[W1231 09:59:21.317848155 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank24]:[W1231 09:59:21.324640667 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank25]:[W1231 09:59:21.324669947 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank27]:[W1231 09:59:21.330098113 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank26]:[W1231 09:59:21.330098283 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank28]:[W1231 09:59:21.353938196 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank29]:[W1231 09:59:21.353938396 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank30]:[W1231 09:59:21.364899299 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank31]:[W1231 09:59:21.364899069 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[2025-12-31 09:59:26 DP5 TP20 EP20] Capture draft cuda graph end. Time elapsed: 6.93 s. mem usage=0.26 GB. avail mem=8.04 GB.
[2025-12-31 09:59:26 DP5 TP20 EP20] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.04 GB
[2025-12-31 09:59:26 DP5 TP21 EP21] Capture draft cuda graph end. Time elapsed: 6.96 s. mem usage=0.26 GB. avail mem=8.26 GB.
[2025-12-31 09:59:26 DP5 TP21 EP21] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.26 GB
[2025-12-31 09:59:26 DP7 TP28 EP28] Capture draft cuda graph end. Time elapsed: 7.01 s. mem usage=0.26 GB. avail mem=8.03 GB.
[2025-12-31 09:59:26 DP7 TP28 EP28] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.03 GB
[2025-12-31 09:59:26 DP6 TP24 EP24] Capture draft cuda graph end. Time elapsed: 6.99 s. mem usage=0.26 GB. avail mem=8.03 GB.
[2025-12-31 09:59:26 DP6 TP24 EP24] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.03 GB
[2025-12-31 09:59:26 DP5 TP22 EP22] Capture draft cuda graph end. Time elapsed: 7.05 s. mem usage=0.26 GB. avail mem=8.04 GB.
[2025-12-31 09:59:26 DP6 TP25 EP25] Capture draft cuda graph end. Time elapsed: 7.07 s. mem usage=0.26 GB. avail mem=8.27 GB.
[2025-12-31 09:59:26 DP5 TP22 EP22] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.04 GB
[2025-12-31 09:59:26 DP6 TP25 EP25] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.27 GB
[2025-12-31 09:59:26 DP5 TP23 EP23] Capture draft cuda graph end. Time elapsed: 7.10 s. mem usage=0.26 GB. avail mem=8.26 GB.
[2025-12-31 09:59:26 DP5 TP23 EP23] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.26 GB
[2025-12-31 09:59:26 DP7 TP31 EP31] Capture draft cuda graph end. Time elapsed: 7.08 s. mem usage=0.26 GB. avail mem=8.26 GB.
[2025-12-31 09:59:26 DP7 TP31 EP31] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.26 GB
[2025-12-31 09:59:26 DP7 TP30 EP30] Capture draft cuda graph end. Time elapsed: 7.07 s. mem usage=0.26 GB. avail mem=8.04 GB.
[2025-12-31 09:59:26 DP7 TP30 EP30] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.04 GB
[2025-12-31 09:59:26 DP4 TP16 EP16] Capture draft cuda graph end. Time elapsed: 7.09 s. mem usage=0.26 GB. avail mem=7.96 GB.
[2025-12-31 09:59:26 DP4 TP16 EP16] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=7.96 GB
[2025-12-31 09:59:26 DP4 TP17 EP17] Capture draft cuda graph end. Time elapsed: 7.06 s. mem usage=0.26 GB. avail mem=8.27 GB.
[2025-12-31 09:59:26 DP6 TP26 EP26] Capture draft cuda graph end. Time elapsed: 7.11 s. mem usage=0.26 GB. avail mem=8.04 GB.
[2025-12-31 09:59:26 DP4 TP17 EP17] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.27 GB
[2025-12-31 09:59:26 DP6 TP26 EP26] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.04 GB
[2025-12-31 09:59:26 DP6 TP27 EP27] Capture draft cuda graph end. Time elapsed: 7.19 s. mem usage=0.26 GB. avail mem=8.26 GB.
[2025-12-31 09:59:26 DP6 TP27 EP27] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.26 GB
[2025-12-31 09:59:26 DP7 TP29 EP29] Capture draft cuda graph end. Time elapsed: 7.18 s. mem usage=0.26 GB. avail mem=8.26 GB.
[2025-12-31 09:59:26 DP7 TP29 EP29] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.26 GB
[2025-12-31 09:59:26 DP4 TP18 EP18] Capture draft cuda graph end. Time elapsed: 7.20 s. mem usage=0.26 GB. avail mem=8.04 GB.
[2025-12-31 09:59:26 DP4 TP18 EP18] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.04 GB
[2025-12-31 09:59:26 DP4 TP19 EP19] Capture draft cuda graph end. Time elapsed: 7.24 s. mem usage=0.26 GB. avail mem=8.26 GB.
[2025-12-31 09:59:26 DP4 TP19 EP19] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.26 GB
[2025-12-31 09:59:28 DP5 TP20 EP20] Capture draft extend cuda graph end. Time elapsed: 1.99 s. mem usage=0.12 GB. avail mem=7.91 GB.
[2025-12-31 09:59:28 DP5 TP21 EP21] Capture draft extend cuda graph end. Time elapsed: 1.98 s. mem usage=0.12 GB. avail mem=8.14 GB.
[2025-12-31 09:59:28 DP7 TP28 EP28] Capture draft extend cuda graph end. Time elapsed: 1.98 s. mem usage=0.12 GB. avail mem=7.91 GB.
[2025-12-31 09:59:28 DP6 TP24 EP24] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=7.90 GB.
[2025-12-31 09:59:28 DP5 TP22 EP22] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=7.92 GB.
[2025-12-31 09:59:28 DP6 TP25 EP25] Capture draft extend cuda graph end. Time elapsed: 1.98 s. mem usage=0.12 GB. avail mem=8.15 GB.
[2025-12-31 09:59:28 DP5 TP23 EP23] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=8.14 GB.
[2025-12-31 09:59:28 DP7 TP31 EP31] Capture draft extend cuda graph end. Time elapsed: 1.98 s. mem usage=0.12 GB. avail mem=8.14 GB.
[2025-12-31 09:59:28 DP7 TP30 EP30] Capture draft extend cuda graph end. Time elapsed: 1.98 s. mem usage=0.12 GB. avail mem=7.91 GB.
[2025-12-31 09:59:28 DP4 TP16 EP16] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=7.84 GB.
[2025-12-31 09:59:28 DP4 TP17 EP17] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=8.14 GB.
[2025-12-31 09:59:28 DP6 TP26 EP26] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=7.91 GB.
[2025-12-31 09:59:28 DP7 TP29 EP29] Capture draft extend cuda graph end. Time elapsed: 1.95 s. mem usage=0.12 GB. avail mem=8.14 GB.
[2025-12-31 09:59:28 DP6 TP27 EP27] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=8.14 GB.
[2025-12-31 09:59:28 DP4 TP18 EP18] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=7.92 GB.
[2025-12-31 09:59:28 DP4 TP19 EP19] Capture draft extend cuda graph end. Time elapsed: 1.97 s. mem usage=0.12 GB. avail mem=8.14 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=215, family=2, type=2, proto=0, laddr=('192.168.0.77', 59017), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 38428), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=226, family=2, type=2, proto=0, laddr=('192.168.0.77', 34716), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 52459), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP5 TP20 EP20] Invalid or no transfer protocol specified, using default protocol.
[2025-12-31 09:59:28 DP5 TP21 EP21] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=215, family=2, type=2, proto=0, laddr=('192.168.0.77', 52640), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=215, family=2, type=2, proto=0, laddr=('192.168.0.77', 49061), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=226, family=2, type=2, proto=0, laddr=('192.168.0.77', 36102), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP6 TP24 EP24] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=226, family=2, type=2, proto=0, laddr=('192.168.0.77', 43046), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP7 TP28 EP28] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 52728), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 35060), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP5 TP22 EP22] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 36276), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 58329), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 36044), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 37083), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP6 TP25 EP25] Invalid or no transfer protocol specified, using default protocol.
[2025-12-31 09:59:28 DP5 TP23 EP23] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 44857), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 60612), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 56082), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 58018), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP7 TP30 EP30] Invalid or no transfer protocol specified, using default protocol.
[2025-12-31 09:59:28 DP7 TP31 EP31] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=215, family=2, type=2, proto=0, laddr=('192.168.0.77', 51714), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=226, family=2, type=2, proto=0, laddr=('192.168.0.77', 57989), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP4 TP16 EP16] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 53444), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 57421), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP4 TP17 EP17] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 57085), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 42229), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP6 TP26 EP26] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 37213), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 59525), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP7 TP29 EP29] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 35319), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 55006), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP6 TP27 EP27] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 59363), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 56358), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:28 DP4 TP18 EP18] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=198, family=2, type=2, proto=0, laddr=('192.168.0.77', 36307), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=205, family=2, type=2, proto=0, laddr=('192.168.0.77', 48507), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-31 09:59:29 DP4 TP19 EP19] Invalid or no transfer protocol specified, using default protocol.
[2025-12-31 09:59:39] Dummy health check server started in background thread at 192.168.0.77:8000
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2025-12-31 10:02:41 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 0.15, #queue-req: 0, 
[2025-12-31 10:02:42 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.10, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 0.17, #queue-req: 0, 
[2025-12-31 10:02:45 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 28.96, #queue-req: 0, 
[2025-12-31 10:02:46 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 29.67, #queue-req: 0, 
[2025-12-31 10:02:46 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 0.16, #queue-req: 0, 
[2025-12-31 10:02:48 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.85, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.81, #queue-req: 0, 
[2025-12-31 10:02:49 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.02, #queue-req: 0, 
[2025-12-31 10:02:49 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.82, #queue-req: 0, 
[2025-12-31 10:02:49 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.52, accept rate: 0.63, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 0.13, #queue-req: 0, 
[2025-12-31 10:02:51 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.56, #queue-req: 0, 
[2025-12-31 10:02:51 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.17, #queue-req: 0, 
[2025-12-31 10:02:51 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.88, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-31 10:02:52 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.10, #queue-req: 0, 
[2025-12-31 10:02:54 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.60, accept rate: 0.65, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.44, #queue-req: 0, 
[2025-12-31 10:03:01 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.60, accept rate: 0.90, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.78, #queue-req: 0, 
[2025-12-31 10:03:01 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.65, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 10.88, #queue-req: 0, 
[2025-12-31 10:03:08 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 6.17, #queue-req: 0, 
[2025-12-31 10:03:19 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 193024, token usage: 0.88, accept len: 2.93, accept rate: 0.73, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 6.28, #queue-req: 0, 
[2025-12-31 10:03:20 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193152, token usage: 0.88, accept len: 3.29, accept rate: 0.82, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.01, #queue-req: 0, 
[2025-12-31 10:03:20 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 192896, token usage: 0.88, accept len: 2.81, accept rate: 0.70, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 11.66, #queue-req: 0, 
[2025-12-31 10:03:20 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 192768, token usage: 0.87, accept len: 3.46, accept rate: 0.87, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 21.94, #queue-req: 0, 
[2025-12-31 10:03:28 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.94, #queue-req: 0, 
[2025-12-31 10:03:28 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193408, token usage: 0.88, accept len: 3.45, accept rate: 0.86, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 33.74, #queue-req: 0, 
[2025-12-31 10:03:28 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193152, token usage: 0.88, accept len: 2.61, accept rate: 0.65, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 25.55, #queue-req: 0, 
[2025-12-31 10:03:30 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193152, token usage: 0.88, accept len: 3.70, accept rate: 0.93, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 29.81, #queue-req: 0, 
[2025-12-31 10:03:48 DP6 TP24 EP24] Decode batch, #running-req: 3, #token: 193408, token usage: 0.88, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.00, #prealloc-req: 1, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.85, #queue-req: 0, 
[2025-12-31 10:03:54 DP4 TP16 EP16] Decode batch, #running-req: 3, #token: 193664, token usage: 0.88, accept len: 3.09, accept rate: 0.77, pre-allocated usage: 0.00, #prealloc-req: 1, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.10, #queue-req: 0, 
[2025-12-31 10:03:54 DP5 TP20 EP20] Decode batch, #running-req: 3, #token: 193536, token usage: 0.88, accept len: 2.97, accept rate: 0.74, pre-allocated usage: 0.00, #prealloc-req: 1, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.31, #queue-req: 0, 
[2025-12-31 10:03:54 DP7 TP28 EP28] Decode batch, #running-req: 3, #token: 193536, token usage: 0.88, accept len: 3.51, accept rate: 0.88, pre-allocated usage: 0.00, #prealloc-req: 1, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 16.27, #queue-req: 0, 
[2025-12-31 10:03:56 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 192768, token usage: 0.87, accept len: 3.03, accept rate: 0.76, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.65, #queue-req: 0, 
[2025-12-31 10:03:57 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193024, token usage: 0.88, accept len: 3.61, accept rate: 0.90, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 102.28, #queue-req: 0, 
[2025-12-31 10:03:57 DP5 TP20 EP20] Decode batch, #running-req: 3, #token: 193920, token usage: 0.88, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.00, #prealloc-req: 2, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 127.66, #queue-req: 0, 
[2025-12-31 10:03:57 DP7 TP28 EP28] Decode batch, #running-req: 3, #token: 193920, token usage: 0.88, accept len: 3.52, accept rate: 0.88, pre-allocated usage: 0.00, #prealloc-req: 1, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 142.67, #queue-req: 0, 
[2025-12-31 10:03:59 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 193024, token usage: 0.88, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 81.17, #queue-req: 0, 
[2025-12-31 10:03:59 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193408, token usage: 0.88, accept len: 3.61, accept rate: 0.90, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 97.63, #queue-req: 0, 
[2025-12-31 10:04:00 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 125.33, #queue-req: 0, 
[2025-12-31 10:04:00 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 3.35, accept rate: 0.84, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 115.62, #queue-req: 0, 
[2025-12-31 10:04:02 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 2.96, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 80.15, #queue-req: 0, 
[2025-12-31 10:04:02 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193536, token usage: 0.88, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.00, #prealloc-req: 1, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 69.09, #queue-req: 0, 
[2025-12-31 10:04:02 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193408, token usage: 0.88, accept len: 3.26, accept rate: 0.82, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 88.35, #queue-req: 0, 
[2025-12-31 10:04:03 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193536, token usage: 0.88, accept len: 3.14, accept rate: 0.78, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 84.96, #queue-req: 0, 
[2025-12-31 10:04:05 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 193536, token usage: 0.88, accept len: 2.95, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 80.01, #queue-req: 0, 
[2025-12-31 10:04:05 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193792, token usage: 0.88, accept len: 3.24, accept rate: 0.81, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 87.85, #queue-req: 0, 
[2025-12-31 10:04:05 DP4 TP16 EP16] Decode batch, #running-req: 3, #token: 193792, token usage: 0.88, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.00, #prealloc-req: 1, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 101.19, #queue-req: 0, 
[2025-12-31 10:04:06 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193792, token usage: 0.88, accept len: 3.16, accept rate: 0.79, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 85.85, #queue-req: 0, 
[2025-12-31 10:04:08 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 193792, token usage: 0.88, accept len: 3.01, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 81.58, #queue-req: 0, 
[2025-12-31 10:04:08 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 194048, token usage: 0.88, accept len: 3.36, accept rate: 0.84, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 91.06, #queue-req: 0, 
[2025-12-31 10:04:08 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193152, token usage: 0.88, accept len: 2.31, accept rate: 0.58, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 93.08, #queue-req: 0, 
[2025-12-31 10:04:09 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193024, token usage: 0.88, accept len: 3.11, accept rate: 0.78, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 97.43, #queue-req: 0, 
[2025-12-31 10:04:11 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 193920, token usage: 0.88, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 74.05, #queue-req: 0, 
[2025-12-31 10:04:11 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 2.96, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 80.57, #queue-req: 0, 
[2025-12-31 10:04:11 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 80.26, #queue-req: 0, 
[2025-12-31 10:04:12 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 3.01, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 81.90, #queue-req: 0, 
[2025-12-31 10:04:14 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 194176, token usage: 0.88, accept len: 2.95, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 80.26, #queue-req: 0, 
[2025-12-31 10:04:14 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193536, token usage: 0.88, accept len: 3.01, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 81.98, #queue-req: 0, 
[2025-12-31 10:04:14 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 62.61, #queue-req: 0, 
[2025-12-31 10:04:15 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193536, token usage: 0.88, accept len: 3.09, accept rate: 0.77, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 84.07, #queue-req: 0, 
[2025-12-31 10:04:17 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193792, token usage: 0.88, accept len: 2.61, accept rate: 0.65, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 71.79, #queue-req: 0, 
[2025-12-31 10:04:17 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.47, #queue-req: 0, 
[2025-12-31 10:04:17 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.21, accept rate: 0.80, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 63.94, #queue-req: 0, 
[2025-12-31 10:04:19 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.87, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.52, #queue-req: 0, 
[2025-12-31 10:04:20 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.74, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 47.26, #queue-req: 0, 
[2025-12-31 10:04:20 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.01, #queue-req: 0, 
[2025-12-31 10:04:20 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-31 10:04:22 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.08, accept rate: 0.77, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.51, #queue-req: 0, 
[2025-12-31 10:04:23 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.65, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.66, #queue-req: 0, 
[2025-12-31 10:04:23 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.89, #queue-req: 0, 
[2025-12-31 10:04:23 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.35, #queue-req: 0, 
[2025-12-31 10:04:25 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.08, accept rate: 0.77, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.67, #queue-req: 0, 
[2025-12-31 10:04:26 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.26, #queue-req: 0, 
[2025-12-31 10:04:26 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.23, #queue-req: 0, 
[2025-12-31 10:04:26 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.77, #queue-req: 0, 
[2025-12-31 10:04:28 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.70, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.45, #queue-req: 0, 
[2025-12-31 10:04:29 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.38, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.75, #queue-req: 0, 
[2025-12-31 10:04:29 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.65, #queue-req: 0, 
[2025-12-31 10:04:31 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.77, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.45, #queue-req: 0, 
[2025-12-31 10:04:32 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.99, #queue-req: 0, 
[2025-12-31 10:04:34 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.88, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.83, #queue-req: 0, 
[2025-12-31 10:04:36 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.85, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.54, #queue-req: 0, 
[2025-12-31 10:04:38 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 9.41, #queue-req: 0, 
[2025-12-31 10:04:39 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.42, accept rate: 0.86, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 47.52, #queue-req: 0, 
[2025-12-31 10:04:40 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.46, #queue-req: 0, 
[2025-12-31 10:04:42 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 12.22, #queue-req: 0, 
[2025-12-31 10:04:43 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.68, #queue-req: 0, 
[2025-12-31 10:04:45 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.30, accept rate: 0.57, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.09, #queue-req: 0, 
[2025-12-31 10:04:46 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.05, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 18.48, #queue-req: 0, 
[2025-12-31 10:04:46 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.77, accept rate: 0.94, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 52.52, #queue-req: 0, 
[2025-12-31 10:04:48 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.88, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.91, #queue-req: 0, 
[2025-12-31 10:04:49 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.71, #queue-req: 0, 
[2025-12-31 10:04:49 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.08, accept rate: 0.77, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.70, #queue-req: 0, 
[2025-12-31 10:04:51 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.48, accept rate: 0.62, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 34.46, #queue-req: 0, 
[2025-12-31 10:04:52 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.67, #queue-req: 0, 
[2025-12-31 10:04:52 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 47.41, #queue-req: 0, 
[2025-12-31 10:04:52 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 1.45, accept rate: 0.36, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 2.51, #queue-req: 0, 
[2025-12-31 10:04:53 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.62, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.54, #queue-req: 0, 
[2025-12-31 10:04:55 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.08, accept rate: 0.77, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.75, #queue-req: 0, 
[2025-12-31 10:04:55 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.58, accept rate: 0.89, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 49.67, #queue-req: 0, 
[2025-12-31 10:04:55 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.12, accept rate: 0.53, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 29.52, #queue-req: 0, 
[2025-12-31 10:04:56 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 35.38, #queue-req: 0, 
[2025-12-31 10:04:57 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.08, accept rate: 0.77, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.63, #queue-req: 0, 
[2025-12-31 10:04:58 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.98, #queue-req: 0, 
[2025-12-31 10:04:58 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.34, #queue-req: 0, 
[2025-12-31 10:04:59 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.33, accept rate: 0.58, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.14, #queue-req: 0, 
[2025-12-31 10:05:02 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.10, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.78, #queue-req: 0, 
[2025-12-31 10:05:03 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.23, accept rate: 0.81, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 25.64, #queue-req: 0, 
[2025-12-31 10:05:04 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.17, accept rate: 0.54, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 17.29, #queue-req: 0, 
[2025-12-31 10:05:05 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.78, #queue-req: 0, 
[2025-12-31 10:05:06 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.83, #queue-req: 0, 
[2025-12-31 10:05:07 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.23, accept rate: 0.56, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 30.72, #queue-req: 0, 
[2025-12-31 10:05:08 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.13, #queue-req: 0, 
[2025-12-31 10:05:09 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 1.07, accept rate: 0.27, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.78, #queue-req: 0, 
[2025-12-31 10:05:10 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.45, accept rate: 0.61, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 33.65, #queue-req: 0, 
[2025-12-31 10:05:11 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.33, accept rate: 0.83, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 9.89, #queue-req: 0, 
[2025-12-31 10:05:12 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 1.02, accept rate: 0.26, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.09, #queue-req: 0, 
[2025-12-31 10:05:13 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 2.29, accept rate: 0.57, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.13, #queue-req: 0, 
[2025-12-31 10:05:14 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.79, #queue-req: 0, 
[2025-12-31 10:05:15 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.71, #queue-req: 0, 
[2025-12-31 10:05:16 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.86, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.25, #queue-req: 0, 
[2025-12-31 10:05:17 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.95, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.72, #queue-req: 0, 
[2025-12-31 10:05:17 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.43, #queue-req: 0, 
[2025-12-31 10:05:17 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 1.15, accept rate: 0.29, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 15.87, #queue-req: 0, 
[2025-12-31 10:05:19 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.90, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.86, #queue-req: 0, 
[2025-12-31 10:05:20 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.08, accept rate: 0.77, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.22, #queue-req: 0, 
[2025-12-31 10:05:20 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.67, accept rate: 0.67, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.72, #queue-req: 0, 
[2025-12-31 10:05:20 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193024, token usage: 0.88, accept len: 1.55, accept rate: 0.39, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 26.45, #queue-req: 0, 
[2025-12-31 10:05:25 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 16.37, #queue-req: 0, 
[2025-12-31 10:05:26 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 19.42, #queue-req: 0, 
[2025-12-31 10:05:26 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.38, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 21.66, #queue-req: 0, 
[2025-12-31 10:05:27 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193152, token usage: 0.88, accept len: 1.82, accept rate: 0.46, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 23.41, #queue-req: 0, 
[2025-12-31 10:05:28 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.40, accept rate: 0.60, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.81, #queue-req: 0, 
[2025-12-31 10:05:31 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.58, accept rate: 0.89, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 26.82, #queue-req: 0, 
[2025-12-31 10:05:31 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 19.13, #queue-req: 0, 
[2025-12-31 10:05:32 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 2.05, accept rate: 0.51, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 30.76, #queue-req: 0, 
[2025-12-31 10:05:33 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.62, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 19.71, #queue-req: 0, 
[2025-12-31 10:05:36 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.77, accept rate: 0.94, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.29, #queue-req: 0, 
[2025-12-31 10:05:36 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 23.31, #queue-req: 0, 
[2025-12-31 10:05:37 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193408, token usage: 0.88, accept len: 1.94, accept rate: 0.48, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 33.15, #queue-req: 0, 
[2025-12-31 10:05:38 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 23.32, #queue-req: 0, 
[2025-12-31 10:05:39 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.48, accept rate: 0.62, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 33.90, #queue-req: 0, 
[2025-12-31 10:05:39 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.20, #queue-req: 0, 
[2025-12-31 10:05:40 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193536, token usage: 0.88, accept len: 1.90, accept rate: 0.47, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 52.08, #queue-req: 0, 
[2025-12-31 10:05:41 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.85, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.06, #queue-req: 0, 
[2025-12-31 10:05:42 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.45, accept rate: 0.86, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 47.27, #queue-req: 0, 
[2025-12-31 10:05:42 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128256, token usage: 0.58, accept len: 2.99, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 70.85, #queue-req: 0, 
[2025-12-31 10:05:43 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193664, token usage: 0.88, accept len: 2.20, accept rate: 0.55, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 60.33, #queue-req: 0, 
[2025-12-31 10:05:44 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193152, token usage: 0.88, accept len: 2.86, accept rate: 0.71, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.72, #queue-req: 0, 
[2025-12-31 10:05:45 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.10, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.41, #queue-req: 0, 
[2025-12-31 10:05:45 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128384, token usage: 0.58, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.70, #queue-req: 0, 
[2025-12-31 10:05:45 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193920, token usage: 0.88, accept len: 2.05, accept rate: 0.51, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 56.13, #queue-req: 0, 
[2025-12-31 10:05:47 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128384, token usage: 0.58, accept len: 3.19, accept rate: 0.80, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 86.52, #queue-req: 0, 
[2025-12-31 10:05:48 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128512, token usage: 0.58, accept len: 2.88, accept rate: 0.72, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.52, #queue-req: 0, 
[2025-12-31 10:05:48 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 194048, token usage: 0.88, accept len: 2.15, accept rate: 0.54, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 59.09, #queue-req: 0, 
[2025-12-31 10:05:49 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128256, token usage: 0.58, accept len: 3.52, accept rate: 0.88, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.57, #queue-req: 0, 
[2025-12-31 10:05:50 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128512, token usage: 0.58, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.27, #queue-req: 0, 
[2025-12-31 10:05:51 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128640, token usage: 0.58, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.88, #queue-req: 0, 
[2025-12-31 10:05:51 DP7 TP28 EP28] Decode batch, #running-req: 3, #token: 129152, token usage: 0.59, accept len: 2.31, accept rate: 0.58, pre-allocated usage: 0.00, #prealloc-req: 1, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 72.00, #queue-req: 0, 
[2025-12-31 10:05:52 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128384, token usage: 0.58, accept len: 3.90, accept rate: 0.97, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 53.59, #queue-req: 0, 
[2025-12-31 10:05:53 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128640, token usage: 0.58, accept len: 2.85, accept rate: 0.71, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.18, #queue-req: 0, 
[2025-12-31 10:05:54 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128768, token usage: 0.58, accept len: 3.08, accept rate: 0.77, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.30, #queue-req: 0, 
[2025-12-31 10:05:54 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128384, token usage: 0.58, accept len: 2.89, accept rate: 0.72, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.74, #queue-req: 0, 
[2025-12-31 10:05:55 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128640, token usage: 0.58, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.07, #queue-req: 0, 
[2025-12-31 10:05:56 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128768, token usage: 0.58, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.43, #queue-req: 0, 
[2025-12-31 10:05:57 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.35, #queue-req: 0, 
[2025-12-31 10:05:57 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128512, token usage: 0.58, accept len: 3.10, accept rate: 0.78, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.71, #queue-req: 0, 
[2025-12-31 10:05:57 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128768, token usage: 0.58, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.10, #queue-req: 0, 
[2025-12-31 10:05:58 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.59, #queue-req: 0, 
[2025-12-31 10:05:59 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 129024, token usage: 0.59, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.88, #queue-req: 0, 
[2025-12-31 10:06:00 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128640, token usage: 0.58, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.39, #queue-req: 0, 
[2025-12-31 10:06:00 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.03, #queue-req: 0, 
[2025-12-31 10:06:01 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 2.77, accept rate: 0.69, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.23, #queue-req: 0, 
[2025-12-31 10:06:02 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 129152, token usage: 0.59, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.78, #queue-req: 0, 
[2025-12-31 10:06:03 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128768, token usage: 0.58, accept len: 2.85, accept rate: 0.71, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.27, #queue-req: 0, 
[2025-12-31 10:06:03 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 129024, token usage: 0.59, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.10, #queue-req: 0, 
[2025-12-31 10:06:04 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 129024, token usage: 0.59, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.56, #queue-req: 0, 
[2025-12-31 10:06:06 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.09, #queue-req: 0, 
[2025-12-31 10:06:07 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 129152, token usage: 0.59, accept len: 2.95, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.75, #queue-req: 0, 
[2025-12-31 10:06:09 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 129024, token usage: 0.59, accept len: 3.23, accept rate: 0.81, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.62, #queue-req: 0, 
[2025-12-31 10:06:12 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 129152, token usage: 0.59, accept len: 3.23, accept rate: 0.81, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.84, #queue-req: 0, 
[2025-12-31 10:06:12 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 2.52, accept rate: 0.63, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 10.06, #queue-req: 0, 
[2025-12-31 10:06:15 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 1.65, accept rate: 0.41, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 22.90, #queue-req: 0, 
[2025-12-31 10:06:17 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 12.76, #queue-req: 0, 
[2025-12-31 10:06:18 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 2.52, accept rate: 0.63, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 34.95, #queue-req: 0, 
[2025-12-31 10:06:20 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 8.23, #queue-req: 0, 
[2025-12-31 10:06:20 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.61, #queue-req: 0, 
[2025-12-31 10:06:21 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 35.34, #queue-req: 0, 
[2025-12-31 10:06:23 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 1.52, accept rate: 0.38, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 21.16, #queue-req: 0, 
[2025-12-31 10:06:23 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.06, #queue-req: 0, 
[2025-12-31 10:06:24 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 1.85, accept rate: 0.46, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 25.62, #queue-req: 0, 
[2025-12-31 10:06:24 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 2.67, accept rate: 0.67, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 8.57, #queue-req: 0, 
[2025-12-31 10:06:26 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 1.80, accept rate: 0.45, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.93, #queue-req: 0, 
[2025-12-31 10:06:26 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.79, #queue-req: 0, 
[2025-12-31 10:06:27 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 2.58, accept rate: 0.64, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 35.75, #queue-req: 0, 
[2025-12-31 10:06:27 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.17, #queue-req: 0, 
[2025-12-31 10:06:28 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 1.80, accept rate: 0.45, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.95, #queue-req: 0, 
[2025-12-31 10:06:29 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.30, #queue-req: 0, 
[2025-12-31 10:06:30 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 2.52, accept rate: 0.63, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 34.95, #queue-req: 0, 
[2025-12-31 10:06:30 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.19, #queue-req: 0, 
[2025-12-31 10:06:31 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 1.93, accept rate: 0.48, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 26.65, #queue-req: 0, 
[2025-12-31 10:06:31 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.98, #queue-req: 0, 
[2025-12-31 10:06:33 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 1.40, accept rate: 0.35, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 19.37, #queue-req: 0, 
[2025-12-31 10:06:33 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 2.65, accept rate: 0.66, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.66, #queue-req: 0, 
[2025-12-31 10:06:34 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 1.80, accept rate: 0.45, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.88, #queue-req: 0, 
[2025-12-31 10:06:34 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.81, #queue-req: 0, 
[2025-12-31 10:06:36 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 1.85, accept rate: 0.46, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 25.57, #queue-req: 0, 
[2025-12-31 10:06:36 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 3.60, accept rate: 0.90, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 49.76, #queue-req: 0, 
[2025-12-31 10:06:37 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 2.12, accept rate: 0.53, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 29.32, #queue-req: 0, 
[2025-12-31 10:06:37 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.45, #queue-req: 0, 
[2025-12-31 10:06:38 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 3.50, accept rate: 0.88, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.33, #queue-req: 0, 
[2025-12-31 10:06:39 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.86, #queue-req: 0, 
[2025-12-31 10:06:40 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.79, #queue-req: 0, 
[2025-12-31 10:06:41 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 2.45, accept rate: 0.61, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 34.04, #queue-req: 0, 
[2025-12-31 10:06:43 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 2.35, accept rate: 0.59, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.70, #queue-req: 0, 
[2025-12-31 10:06:44 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 2.58, accept rate: 0.64, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 35.84, #queue-req: 0, 
[2025-12-31 10:06:46 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 2.08, accept rate: 0.52, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 28.86, #queue-req: 0, 
[2025-12-31 10:06:47 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 35.51, #queue-req: 0, 
[2025-12-31 10:06:49 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 1.93, accept rate: 0.48, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 26.82, #queue-req: 0, 
[2025-12-31 10:06:51 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 2.30, accept rate: 0.57, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.10, #queue-req: 0, 
[2025-12-31 10:06:54 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 1.82, accept rate: 0.46, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 25.71, #queue-req: 0, 
[2025-12-31 10:07:56 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.40, accept rate: 0.60, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 1.55, #queue-req: 0, 
[2025-12-31 10:07:59 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.88, #queue-req: 0, 
[2025-12-31 10:08:00 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 1.52, #queue-req: 0, 
[2025-12-31 10:08:01 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 1.93, #queue-req: 0, 
[2025-12-31 10:08:02 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.93, #queue-req: 0, 
[2025-12-31 10:08:03 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.95, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.18, #queue-req: 0, 
[2025-12-31 10:08:04 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.60, #queue-req: 0, 
[2025-12-31 10:08:05 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.76, #queue-req: 0, 
[2025-12-31 10:08:05 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.05, accept rate: 0.51, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 1.06, #queue-req: 0, 
[2025-12-31 10:08:06 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.05, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.47, #queue-req: 0, 
[2025-12-31 10:08:07 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.55, #queue-req: 0, 
[2025-12-31 10:08:08 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.33, #queue-req: 0, 
[2025-12-31 10:08:08 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.77, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.63, #queue-req: 0, 
[2025-12-31 10:08:09 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.65, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.82, #queue-req: 0, 
[2025-12-31 10:08:10 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.52, accept rate: 0.88, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-12-31 10:08:10 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-31 10:08:11 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.76, #queue-req: 0, 
[2025-12-31 10:08:12 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.60, #queue-req: 0, 
[2025-12-31 10:08:13 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.65, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.41, #queue-req: 0, 
[2025-12-31 10:08:13 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.95, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.73, #queue-req: 0, 
[2025-12-31 10:08:13 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.07, #queue-req: 0, 
[2025-12-31 10:08:15 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.90, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.01, #queue-req: 0, 
[2025-12-31 10:08:16 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.77, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.31, #queue-req: 0, 
[2025-12-31 10:08:16 DP6 TP24 EP24] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 3.09, accept rate: 0.77, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.73, #queue-req: 0, 
[2025-12-31 10:08:16 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.85, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.29, #queue-req: 0, 
[2025-12-31 10:08:17 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.57, #queue-req: 0, 
[2025-12-31 10:08:19 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.62, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.10, #queue-req: 0, 
[2025-12-31 10:08:19 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.32, accept rate: 0.83, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 85.69, #queue-req: 0, 
[2025-12-31 10:08:19 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.89, #queue-req: 0, 
[2025-12-31 10:08:20 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.84, #queue-req: 0, 
[2025-12-31 10:08:21 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.62, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.11, #queue-req: 0, 
[2025-12-31 10:08:22 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.52, accept rate: 0.88, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.51, #queue-req: 0, 
[2025-12-31 10:08:22 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.63, #queue-req: 0, 
[2025-12-31 10:08:24 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.70, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.14, #queue-req: 0, 
[2025-12-31 10:08:25 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.52, accept rate: 0.88, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.50, #queue-req: 0, 
[2025-12-31 10:08:25 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193152, token usage: 0.88, accept len: 3.01, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 68.79, #queue-req: 0, 
[2025-12-31 10:08:28 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.62, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.02, #queue-req: 0, 
[2025-12-31 10:08:28 DP7 TP28 EP28] Decode batch, #running-req: 2, #token: 193408, token usage: 0.88, accept len: 3.10, accept rate: 0.78, pre-allocated usage: 0.29, #prealloc-req: 1, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 85.55, #queue-req: 0, 
[2025-12-31 10:08:31 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.62, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.08, #queue-req: 0, 
[2025-12-31 10:08:31 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.44, accept rate: 0.86, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 72.14, #queue-req: 0, 
[2025-12-31 10:08:34 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.27, accept rate: 0.57, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 31.33, #queue-req: 0, 
[2025-12-31 10:08:34 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.80, accept rate: 0.95, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 52.34, #queue-req: 0, 
[2025-12-31 10:08:34 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.85, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 8.09, #queue-req: 0, 
[2025-12-31 10:08:37 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.00, accept rate: 0.50, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 27.51, #queue-req: 0, 
[2025-12-31 10:08:37 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.88, #queue-req: 0, 
[2025-12-31 10:08:37 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.62, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.15, #queue-req: 0, 
[2025-12-31 10:08:40 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.15, accept rate: 0.54, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 29.58, #queue-req: 0, 
[2025-12-31 10:08:40 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.77, #queue-req: 0, 
[2025-12-31 10:08:40 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.52, accept rate: 0.63, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 34.69, #queue-req: 0, 
[2025-12-31 10:08:43 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.65, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.34, #queue-req: 0, 
[2025-12-31 10:08:43 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.27, accept rate: 0.57, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 31.24, #queue-req: 0, 
[2025-12-31 10:08:45 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.00, accept rate: 0.50, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 27.60, #queue-req: 0, 
[2025-12-31 10:08:46 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.62, accept rate: 0.66, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.24, #queue-req: 0, 
[2025-12-31 10:08:49 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.83, #queue-req: 0, 
[2025-12-31 10:08:52 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.67, accept rate: 0.67, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.93, #queue-req: 0, 
[2025-12-31 10:08:53 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 4.80, #queue-req: 0, 
[2025-12-31 10:08:55 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193024, token usage: 0.88, accept len: 2.77, accept rate: 0.69, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.06, #queue-req: 0, 
[2025-12-31 10:08:56 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.62, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.08, #queue-req: 0, 
[2025-12-31 10:08:58 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 88.05, #queue-req: 0, 
[2025-12-31 10:08:58 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.70, accept rate: 0.93, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 51.29, #queue-req: 0, 
[2025-12-31 10:08:59 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.60, accept rate: 0.65, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 5.28, #queue-req: 0, 
[2025-12-31 10:09:01 DP5 TP20 EP20] Decode batch, #running-req: 2, #token: 193536, token usage: 0.88, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 83.61, #queue-req: 0, 
[2025-12-31 10:09:01 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.73, accept rate: 0.93, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 51.46, #queue-req: 0, 
[2025-12-31 10:09:02 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.87, #queue-req: 0, 
[2025-12-31 10:09:03 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.52, accept rate: 0.63, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 5.82, #queue-req: 0, 
[2025-12-31 10:09:03 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.26, accept rate: 0.81, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 56.34, #queue-req: 0, 
[2025-12-31 10:09:04 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.70, accept rate: 0.93, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 51.15, #queue-req: 0, 
[2025-12-31 10:09:05 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.77, #queue-req: 0, 
[2025-12-31 10:09:06 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.80, #queue-req: 0, 
[2025-12-31 10:09:06 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.44, #queue-req: 0, 
[2025-12-31 10:09:07 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.60, accept rate: 0.90, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 49.55, #queue-req: 0, 
[2025-12-31 10:09:08 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.48, accept rate: 0.62, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 34.08, #queue-req: 0, 
[2025-12-31 10:09:09 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.26, #queue-req: 0, 
[2025-12-31 10:09:09 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.78, #queue-req: 0, 
[2025-12-31 10:09:10 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193280, token usage: 0.88, accept len: 3.31, accept rate: 0.83, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 67.70, #queue-req: 0, 
[2025-12-31 10:09:11 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.70, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.06, #queue-req: 0, 
[2025-12-31 10:09:12 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-31 10:09:12 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.50, accept rate: 0.88, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.14, #queue-req: 0, 
[2025-12-31 10:09:13 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.86, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.39, #queue-req: 0, 
[2025-12-31 10:09:14 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.12, #queue-req: 0, 
[2025-12-31 10:09:14 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.10, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.69, #queue-req: 0, 
[2025-12-31 10:09:15 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.84, #queue-req: 0, 
[2025-12-31 10:09:16 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.59, #queue-req: 0, 
[2025-12-31 10:09:17 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.08, #queue-req: 0, 
[2025-12-31 10:09:17 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.08, #queue-req: 0, 
[2025-12-31 10:09:19 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.02, #queue-req: 0, 
[2025-12-31 10:09:20 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.33, accept rate: 0.83, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.86, #queue-req: 0, 
[2025-12-31 10:09:20 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-31 10:09:22 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.68, #queue-req: 0, 
[2025-12-31 10:09:23 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.05, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.00, #queue-req: 0, 
[2025-12-31 10:09:25 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193024, token usage: 0.88, accept len: 2.65, accept rate: 0.66, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.08, #queue-req: 0, 
[2025-12-31 10:09:28 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193152, token usage: 0.88, accept len: 2.44, accept rate: 0.61, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 67.17, #queue-req: 0, 
[2025-12-31 10:09:30 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 9.06, #queue-req: 0, 
[2025-12-31 10:09:31 DP4 TP16 EP16] Decode batch, #running-req: 2, #token: 193408, token usage: 0.88, accept len: 2.42, accept rate: 0.61, pre-allocated usage: 0.29, #prealloc-req: 2, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 66.52, #queue-req: 0, 
[2025-12-31 10:09:32 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.35, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 10.91, #queue-req: 0, 
[2025-12-31 10:09:32 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.73, accept rate: 0.93, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 51.19, #queue-req: 0, 
[2025-12-31 10:09:33 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.35, accept rate: 0.59, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.67, #queue-req: 0, 
[2025-12-31 10:09:35 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.58, accept rate: 0.89, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 49.18, #queue-req: 0, 
[2025-12-31 10:09:35 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.62, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 49.90, #queue-req: 0, 
[2025-12-31 10:09:35 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 9.85, #queue-req: 0, 
[2025-12-31 10:09:36 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 1.75, accept rate: 0.44, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.09, #queue-req: 0, 
[2025-12-31 10:09:38 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.13, #queue-req: 0, 
[2025-12-31 10:09:38 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.52, accept rate: 0.88, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.61, #queue-req: 0, 
[2025-12-31 10:09:38 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.25, accept rate: 0.81, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.80, #queue-req: 0, 
[2025-12-31 10:09:39 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 1.75, accept rate: 0.44, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.15, #queue-req: 0, 
[2025-12-31 10:09:41 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.21, #queue-req: 0, 
[2025-12-31 10:09:41 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.62, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.07, #queue-req: 0, 
[2025-12-31 10:09:41 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.35, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.26, #queue-req: 0, 
[2025-12-31 10:09:42 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.45, accept rate: 0.86, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 47.67, #queue-req: 0, 
[2025-12-31 10:09:43 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.31, #queue-req: 0, 
[2025-12-31 10:09:44 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.62, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.10, #queue-req: 0, 
[2025-12-31 10:09:44 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.54, #queue-req: 0, 
[2025-12-31 10:09:45 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.75, accept rate: 0.94, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 51.85, #queue-req: 0, 
[2025-12-31 10:09:46 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.32, #queue-req: 0, 
[2025-12-31 10:09:47 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.62, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.15, #queue-req: 0, 
[2025-12-31 10:09:47 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.33, accept rate: 0.83, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.00, #queue-req: 0, 
[2025-12-31 10:09:48 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.30, #queue-req: 0, 
[2025-12-31 10:09:49 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.35, #queue-req: 0, 
[2025-12-31 10:09:50 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.71, #queue-req: 0, 
[2025-12-31 10:09:51 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.65, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 2, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.68, #queue-req: 0, 
[2025-12-31 10:09:53 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.18, #queue-req: 0, 
[2025-12-31 10:09:56 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.23, accept rate: 0.81, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.04, #queue-req: 0, 
[2025-12-31 10:09:57 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.45, accept rate: 0.86, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 22.91, #queue-req: 0, 
[2025-12-31 10:10:00 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.42, accept rate: 0.86, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 47.77, #queue-req: 0, 
[2025-12-31 10:10:00 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 9.77, #queue-req: 0, 
[2025-12-31 10:10:03 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.35, #queue-req: 0, 
[2025-12-31 10:10:03 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.38, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.84, #queue-req: 0, 
[2025-12-31 10:10:05 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.88, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 7.24, #queue-req: 0, 
[2025-12-31 10:10:06 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.42, accept rate: 0.86, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 47.46, #queue-req: 0, 
[2025-12-31 10:10:06 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.52, accept rate: 0.88, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-12-31 10:10:08 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.10, accept rate: 0.53, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 29.00, #queue-req: 0, 
[2025-12-31 10:10:08 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.33, accept rate: 0.83, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 10.74, #queue-req: 0, 
[2025-12-31 10:10:08 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.67, accept rate: 0.92, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.72, #queue-req: 0, 
[2025-12-31 10:10:09 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.38, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.53, #queue-req: 0, 
[2025-12-31 10:10:11 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.55, accept rate: 0.64, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 35.18, #queue-req: 0, 
[2025-12-31 10:10:11 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.15, #queue-req: 0, 
[2025-12-31 10:10:11 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.92, accept rate: 0.73, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-31 10:10:12 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.62, accept rate: 0.91, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 50.06, #queue-req: 0, 
[2025-12-31 10:10:14 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.58, accept rate: 0.64, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 35.61, #queue-req: 0, 
[2025-12-31 10:10:14 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.91, #queue-req: 0, 
[2025-12-31 10:10:14 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.51, #queue-req: 0, 
[2025-12-31 10:10:15 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.18, #queue-req: 0, 
[2025-12-31 10:10:17 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.30, accept rate: 0.57, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 31.80, #queue-req: 0, 
[2025-12-31 10:10:17 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.23, accept rate: 0.81, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.59, #queue-req: 0, 
[2025-12-31 10:10:17 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.67, #queue-req: 0, 
[2025-12-31 10:10:18 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.95, #queue-req: 0, 
[2025-12-31 10:10:20 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.35, accept rate: 0.59, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.41, #queue-req: 0, 
[2025-12-31 10:10:20 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.09, #queue-req: 0, 
[2025-12-31 10:10:23 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.27, #queue-req: 0, 
[2025-12-31 10:10:23 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.38, accept rate: 0.59, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.83, #queue-req: 0, 
[2025-12-31 10:10:25 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-12-31 10:10:25 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 2.70, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.48, #queue-req: 0, 
[2025-12-31 10:10:28 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-12-31 10:10:28 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.42, accept rate: 0.61, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 33.49, #queue-req: 0, 
[2025-12-31 10:10:29 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 9.97, #queue-req: 0, 
[2025-12-31 10:10:31 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.35, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 10.29, #queue-req: 0, 
[2025-12-31 10:10:31 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.58, #prealloc-req: 1, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.65, #queue-req: 0, 
[2025-12-31 10:10:32 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.38, #queue-req: 0, 
[2025-12-31 10:10:34 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.53, #queue-req: 0, 
[2025-12-31 10:10:35 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.33, accept rate: 0.83, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.77, #queue-req: 0, 
[2025-12-31 10:10:36 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 25.67, #queue-req: 0, 
[2025-12-31 10:10:37 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.61, #queue-req: 0, 
[2025-12-31 10:10:38 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.35, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.00, #queue-req: 0, 
[2025-12-31 10:10:39 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.10, accept rate: 0.78, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.53, #queue-req: 0, 
[2025-12-31 10:10:39 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192384, token usage: 0.87, accept len: 3.38, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 12.22, #queue-req: 0, 
[2025-12-31 10:10:40 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.60, accept rate: 0.65, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 35.70, #queue-req: 0, 
[2025-12-31 10:10:41 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.25, accept rate: 0.81, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.72, #queue-req: 0, 
[2025-12-31 10:10:42 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.87, #queue-req: 0, 
[2025-12-31 10:10:42 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192512, token usage: 0.87, accept len: 3.08, accept rate: 0.77, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 42.34, #queue-req: 0, 
[2025-12-31 10:10:42 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.88, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.57, #queue-req: 0, 
[2025-12-31 10:10:44 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.80, #queue-req: 0, 
[2025-12-31 10:10:45 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 2.38, accept rate: 0.59, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 32.64, #queue-req: 0, 
[2025-12-31 10:10:45 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192640, token usage: 0.87, accept len: 3.77, accept rate: 0.94, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 51.82, #queue-req: 0, 
[2025-12-31 10:10:45 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 2.90, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.82, #queue-req: 0, 
[2025-12-31 10:10:46 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.35, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.97, #queue-req: 0, 
[2025-12-31 10:10:48 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.38, #queue-req: 0, 
[2025-12-31 10:10:48 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192768, token usage: 0.87, accept len: 3.38, accept rate: 0.84, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.40, #queue-req: 0, 
[2025-12-31 10:10:48 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.38, #queue-req: 0, 
[2025-12-31 10:10:49 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128256, token usage: 0.58, accept len: 3.25, accept rate: 0.81, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.74, #queue-req: 0, 
[2025-12-31 10:10:51 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193024, token usage: 0.88, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.02, #queue-req: 0, 
[2025-12-31 10:10:51 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 192896, token usage: 0.88, accept len: 3.25, accept rate: 0.81, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.75, #queue-req: 0, 
[2025-12-31 10:10:51 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.86, #queue-req: 0, 
[2025-12-31 10:10:54 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.62, #queue-req: 0, 
[2025-12-31 10:10:54 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 3.50, accept rate: 0.88, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 48.26, #queue-req: 0, 
[2025-12-31 10:10:54 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.78, #queue-req: 0, 
[2025-12-31 10:10:56 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 193280, token usage: 0.88, accept len: 2.90, accept rate: 0.72, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.04, #queue-req: 0, 
[2025-12-31 10:10:57 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 193152, token usage: 0.88, accept len: 2.80, accept rate: 0.70, pre-allocated usage: 0.58, #prealloc-req: 0, #transfer-req: 2, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.69, #queue-req: 0, 
[2025-12-31 10:10:59 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128384, token usage: 0.58, accept len: 3.17, accept rate: 0.79, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 12.88, #queue-req: 0, 
[2025-12-31 10:11:02 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128512, token usage: 0.58, accept len: 3.15, accept rate: 0.79, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.62, #queue-req: 0, 
[2025-12-31 10:11:02 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128256, token usage: 0.58, accept len: 2.73, accept rate: 0.68, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.34, #queue-req: 0, 
[2025-12-31 10:11:05 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128256, token usage: 0.58, accept len: 2.85, accept rate: 0.71, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.47, #queue-req: 0, 
[2025-12-31 10:11:05 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128512, token usage: 0.58, accept len: 2.88, accept rate: 0.72, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.76, #queue-req: 0, 
[2025-12-31 10:11:05 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128384, token usage: 0.58, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.08, #queue-req: 0, 
[2025-12-31 10:11:08 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128384, token usage: 0.58, accept len: 3.33, accept rate: 0.83, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.11, #queue-req: 0, 
[2025-12-31 10:11:08 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128640, token usage: 0.58, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.14, #queue-req: 0, 
[2025-12-31 10:11:08 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128512, token usage: 0.58, accept len: 3.00, accept rate: 0.75, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.61, #queue-req: 0, 
[2025-12-31 10:11:09 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128256, token usage: 0.58, accept len: 3.65, accept rate: 0.91, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 12.32, #queue-req: 0, 
[2025-12-31 10:11:11 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128512, token usage: 0.58, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.34, #queue-req: 0, 
[2025-12-31 10:11:11 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128768, token usage: 0.58, accept len: 2.75, accept rate: 0.69, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.06, #queue-req: 0, 
[2025-12-31 10:11:11 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128640, token usage: 0.58, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.85, #queue-req: 0, 
[2025-12-31 10:11:12 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128384, token usage: 0.58, accept len: 3.80, accept rate: 0.95, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 52.54, #queue-req: 0, 
[2025-12-31 10:11:14 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128640, token usage: 0.58, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.19, #queue-req: 0, 
[2025-12-31 10:11:14 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 2.70, accept rate: 0.68, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 37.28, #queue-req: 0, 
[2025-12-31 10:11:14 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128768, token usage: 0.58, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.00, #queue-req: 0, 
[2025-12-31 10:11:14 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128512, token usage: 0.58, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.23, #queue-req: 0, 
[2025-12-31 10:11:16 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128768, token usage: 0.58, accept len: 3.33, accept rate: 0.83, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.87, #queue-req: 0, 
[2025-12-31 10:11:17 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 129024, token usage: 0.59, accept len: 2.90, accept rate: 0.72, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.01, #queue-req: 0, 
[2025-12-31 10:11:17 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 2.90, accept rate: 0.72, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.01, #queue-req: 0, 
[2025-12-31 10:11:17 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128640, token usage: 0.58, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.13, #queue-req: 0, 
[2025-12-31 10:11:19 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 3.40, accept rate: 0.85, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.81, #queue-req: 0, 
[2025-12-31 10:11:19 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 129152, token usage: 0.59, accept len: 3.05, accept rate: 0.76, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.99, #queue-req: 0, 
[2025-12-31 10:11:20 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 2.77, accept rate: 0.69, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 38.20, #queue-req: 0, 
[2025-12-31 10:11:20 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 128896, token usage: 0.58, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.06, #queue-req: 0, 
[2025-12-31 10:11:22 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 129024, token usage: 0.59, accept len: 3.25, accept rate: 0.81, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.71, #queue-req: 0, 
[2025-12-31 10:11:23 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 129024, token usage: 0.59, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.60, #queue-req: 0, 
[2025-12-31 10:11:23 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 129024, token usage: 0.59, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 54.98, #queue-req: 0, 
[2025-12-31 10:11:25 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 129152, token usage: 0.59, accept len: 3.48, accept rate: 0.87, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 47.74, #queue-req: 0, 
[2025-12-31 10:11:25 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 129152, token usage: 0.59, accept len: 2.95, accept rate: 0.74, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-31 10:11:26 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 129152, token usage: 0.59, accept len: 4.00, accept rate: 1.00, pre-allocated usage: 0.29, #prealloc-req: 0, #transfer-req: 1, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 55.02, #queue-req: 0, 
[2025-12-31 10:11:28 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 2.05, accept rate: 0.51, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 9.26, #queue-req: 0, 
[2025-12-31 10:11:31 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.85, #queue-req: 0, 
[2025-12-31 10:11:34 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.87, #queue-req: 0, 
[2025-12-31 10:11:35 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 1.48, accept rate: 0.37, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 6.49, #queue-req: 0, 
[2025-12-31 10:11:37 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 10.88, #queue-req: 0, 
[2025-12-31 10:11:37 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.89, #queue-req: 0, 
[2025-12-31 10:11:37 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 2.95, accept rate: 0.74, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 40.97, #queue-req: 0, 
[2025-12-31 10:11:40 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 3.20, accept rate: 0.80, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-12-31 10:11:40 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.89, #queue-req: 0, 
[2025-12-31 10:11:40 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 2.30, accept rate: 0.57, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 31.96, #queue-req: 0, 
[2025-12-31 10:11:42 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64128, token usage: 0.29, accept len: 2.00, accept rate: 0.50, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 4.89, #queue-req: 0, 
[2025-12-31 10:11:43 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 3.23, accept rate: 0.81, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.00, #queue-req: 0, 
[2025-12-31 10:11:43 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.95, #queue-req: 0, 
[2025-12-31 10:11:43 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 3.30, accept rate: 0.82, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 46.05, #queue-req: 0, 
[2025-12-31 10:11:45 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 1.90, accept rate: 0.47, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 26.56, #queue-req: 0, 
[2025-12-31 10:11:46 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 2.98, accept rate: 0.74, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.61, #queue-req: 0, 
[2025-12-31 10:11:46 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.99, #queue-req: 0, 
[2025-12-31 10:11:46 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 3.12, accept rate: 0.78, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 43.73, #queue-req: 0, 
[2025-12-31 10:11:48 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 1.43, accept rate: 0.36, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 19.95, #queue-req: 0, 
[2025-12-31 10:11:48 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 2.58, accept rate: 0.64, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 36.02, #queue-req: 0, 
[2025-12-31 10:11:48 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.99, #queue-req: 0, 
[2025-12-31 10:11:49 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 2.08, accept rate: 0.52, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 29.00, #queue-req: 0, 
[2025-12-31 10:11:51 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64256, token usage: 0.29, accept len: 1.62, accept rate: 0.41, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 22.66, #queue-req: 0, 
[2025-12-31 10:11:51 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 3.27, accept rate: 0.82, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 45.67, #queue-req: 0, 
[2025-12-31 10:11:51 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.94, #queue-req: 0, 
[2025-12-31 10:11:52 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 2.25, accept rate: 0.56, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 31.35, #queue-req: 0, 
[2025-12-31 10:11:54 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 1.75, accept rate: 0.44, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.27, #queue-req: 0, 
[2025-12-31 10:11:54 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 3.02, accept rate: 0.76, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 41.92, #queue-req: 0, 
[2025-12-31 10:11:54 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64384, token usage: 0.29, accept len: 1.02, accept rate: 0.26, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.20, #queue-req: 0, 
[2025-12-31 10:11:55 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 2.42, accept rate: 0.61, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 33.59, #queue-req: 0, 
[2025-12-31 10:11:57 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 1.80, accept rate: 0.45, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.85, #queue-req: 0, 
[2025-12-31 10:11:57 DP6 TP24 EP24] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.00, #queue-req: 0, 
[2025-12-31 10:11:57 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.80, #queue-req: 0, 
[2025-12-31 10:11:58 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 2.83, accept rate: 0.71, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 39.00, #queue-req: 0, 
[2025-12-31 10:12:00 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 1.80, accept rate: 0.45, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 24.89, #queue-req: 0, 
[2025-12-31 10:12:00 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.83, #queue-req: 0, 
[2025-12-31 10:12:00 DP5 TP20 EP20] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, accept len: 2.42, accept rate: 0.61, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 33.54, #queue-req: 0, 
[2025-12-31 10:12:03 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 1.52, accept rate: 0.38, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 21.21, #queue-req: 0, 
[2025-12-31 10:12:03 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64512, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.91, #queue-req: 0, 
[2025-12-31 10:12:05 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 1.85, accept rate: 0.46, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 25.76, #queue-req: 0, 
[2025-12-31 10:12:06 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.92, #queue-req: 0, 
[2025-12-31 10:12:08 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 2.08, accept rate: 0.52, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 28.91, #queue-req: 0, 
[2025-12-31 10:12:09 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.93, #queue-req: 0, 
[2025-12-31 10:12:11 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 2.27, accept rate: 0.57, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 31.71, #queue-req: 0, 
[2025-12-31 10:12:12 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64640, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 13.95, #queue-req: 0, 
[2025-12-31 10:12:14 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 2.12, accept rate: 0.53, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 29.83, #queue-req: 0, 
[2025-12-31 10:12:14 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.06, #queue-req: 0, 
[2025-12-31 10:12:17 DP7 TP28 EP28] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 2.23, accept rate: 0.56, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 31.44, #queue-req: 0, 
[2025-12-31 10:12:17 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.14, #queue-req: 0, 
[2025-12-31 10:12:20 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64768, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.15, #queue-req: 0, 
[2025-12-31 10:12:23 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.19, #queue-req: 0, 
[2025-12-31 10:12:26 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.31, #queue-req: 0, 
[2025-12-31 10:12:28 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 64896, token usage: 0.29, accept len: 1.02, accept rate: 0.26, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.72, #queue-req: 0, 
[2025-12-31 10:12:31 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.36, #queue-req: 0, 
[2025-12-31 10:12:34 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 1.05, accept rate: 0.26, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 15.07, #queue-req: 0, 
[2025-12-31 10:12:37 DP4 TP16 EP16] Decode batch, #running-req: 1, #token: 65024, token usage: 0.30, accept len: 1.00, accept rate: 0.25, pre-allocated usage: 0.00, #prealloc-req: 0, #transfer-req: 0, #retracted-req: 0, cpu graph: True, gen throughput (token/s): 14.37, #queue-req: 0, 
.
----------------------------------------------------------------------
Ran 1 test in 3600.001s

OK
[2025-12-31 10:49:40 DP4 TP19 EP19] Attempting to reconnect to 192.168.0.81:8995...
[2025-12-31 10:49:40 DP6 TP27 EP27] Attempting to reconnect to 192.168.0.81:8995...
[2025-12-31 10:49:40 DP4 TP18 EP18] Attempting to reconnect to 192.168.0.81:8995...
[2025-12-31 10:49:40 DP6 TP26 EP26] Attempting to reconnect to 192.168.0.81:8995...
[2025-12-31 10:49:40 DP5 TP23 EP23] Attempting to reconnect to 192.168.0.81:8995...
................2025-12-31 10:49:41.425469 error 27260 pid[1807] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.425495 error 27260 pid[1807] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.425500 error 27260 pid[1807] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.425504 error 27260 pid[1807] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.425509 error 27260 pid[1807] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.425512 error 27260 pid[1807] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.425514 error 27260 pid[1807] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.425517 error 27260 pid[1807] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.431354 error 27262 pid[1811] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.431388 error 27262 pid[1811] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.431396 error 27262 pid[1811] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.431401 error 27262 pid[1811] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.431406 error 27262 pid[1811] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.431409 error 27262 pid[1811] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.431415 error 27262 pid[1811] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.431420 error 27262 pid[1811] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.452169 error 27247 pid[1425] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.452194 error 27247 pid[1425] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.452200 error 27247 pid[1425] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.452204 error 27247 pid[1425] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.452209 error 27247 pid[1425] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.452214 error 27247 pid[1425] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.452218 error 27247 pid[1425] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.452221 error 27247 pid[1425] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.452380 error 27246 pid[1426] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.452400 error 27246 pid[1426] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.452406 error 27246 pid[1426] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.452411 error 27246 pid[1426] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.452418 error 27246 pid[1426] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.452423 error 27246 pid[1426] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.452427 error 27246 pid[1426] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.452430 error 27246 pid[1426] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.453875 error 27268 pid[1805] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.453919 error 27268 pid[1805] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.453927 error 27268 pid[1805] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.453933 error 27268 pid[1805] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.453939 error 27268 pid[1805] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.453946 error 27268 pid[1805] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.453954 error 27268 pid[1805] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.453961 error 27268 pid[1805] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.472023 error 27283 pid[1808] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.472045 error 27283 pid[1808] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.472050 error 27283 pid[1808] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.472054 error 27283 pid[1808] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.472058 error 27283 pid[1808] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.472061 error 27283 pid[1808] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.472064 error 27283 pid[1808] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.472067 error 27283 pid[1808] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.481301 error 27284 pid[1806] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.481342 error 27284 pid[1806] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.481351 error 27284 pid[1806] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.481358 error 27284 pid[1806] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.481368 error 27284 pid[1806] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.481377 error 27284 pid[1806] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.481384 error 27284 pid[1806] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.481394 error 27284 pid[1806] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.504739 error 27297 pid[1814] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.504779 error 27297 pid[1814] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.504786 error 27297 pid[1814] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.504792 error 27297 pid[1814] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.504796 error 27297 pid[1814] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.504802 error 27297 pid[1814] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.504807 error 27297 pid[1814] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.504812 error 27297 pid[1814] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.525714 error 27298 pid[1813] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.525738 error 27298 pid[1813] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.525744 error 27298 pid[1813] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.525750 error 27298 pid[1813] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.525755 error 27298 pid[1813] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.525758 error 27298 pid[1813] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.525762 error 27298 pid[1813] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.525767 error 27298 pid[1813] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.533927 error 27305 pid[1421] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.533962 error 27305 pid[1421] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.533969 error 27305 pid[1421] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.533975 error 27305 pid[1421] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.533980 error 27305 pid[1421] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.533986 error 27305 pid[1421] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.533989 error 27305 pid[1421] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.533997 error 27305 pid[1421] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.535648 error 27327 pid[1812] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.535671 error 27327 pid[1812] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.535677 error 27327 pid[1812] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.535682 error 27327 pid[1812] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.535687 error 27327 pid[1812] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.535695 error 27327 pid[1812] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.535700 error 27327 pid[1812] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.535705 error 27327 pid[1812] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.544300 error 27314 pid[1422] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.544324 error 27314 pid[1422] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.544330 error 27314 pid[1422] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.544336 error 27314 pid[1422] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.544340 error 27314 pid[1422] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.544346 error 27314 pid[1422] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.544350 error 27314 pid[1422] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.544354 error 27314 pid[1422] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.549163 error 27331 pid[1810] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.549173 error 27319 pid[1809] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.549188 error 27331 pid[1810] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.549192 error 27319 pid[1809] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.549194 error 27331 pid[1810] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.549197 error 27319 pid[1809] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.549200 error 27331 pid[1810] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.549202 error 27319 pid[1809] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.549205 error 27331 pid[1810] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.549207 error 27319 pid[1809] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.549209 error 27331 pid[1810] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.549212 error 27319 pid[1809] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.549213 error 27331 pid[1810] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.549216 error 27319 pid[1809] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.549217 error 27331 pid[1810] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.549221 error 27319 pid[1809] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.654342 error 27345 pid[1424] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.654381 error 27345 pid[1424] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.654388 error 27345 pid[1424] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.654393 error 27345 pid[1424] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.654400 error 27345 pid[1424] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.654403 error 27345 pid[1424] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.654408 error 27345 pid[1424] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.654413 error 27345 pid[1424] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
2025-12-31 10:49:41.654904 error 27339 pid[1423] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.654928 error 27339 pid[1423] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.654934 error 27339 pid[1423] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/count_for_senders, get null response
2025-12-31 10:49:41.654938 error 27339 pid[1423] [SMEM smem_trans_store_helper.cpp:181] store add(0) for key(count_for_senders) count failed: -602
2025-12-31 10:49:41.654943 error 27339 pid[1423] [AccLink acc_tcp_link_complex_default.h:504] Failed to send message with message type 0 as the link is broken
2025-12-31 10:49:41.654947 error 27339 pid[1423] [SMEM smem_tcp_config_store.cpp:495] send message failed, result: -11
2025-12-31 10:49:41.654951 error 27339 pid[1423] [SMEM smem_tcp_config_store.cpp:312] send add for key: /trans/129/senders_total_slices_count, get null response
2025-12-31 10:49:41.654956 error 27339 pid[1423] [SMEM smem_trans_store_helper.cpp:220] store add(0) for key(senders_total_slices_count) total count failed: -602
[2025-12-31 10:49:42 DP7 TP30 EP30] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:6858: Connection reset by peer

[2025-12-31 10:49:42 DP7 TP31 EP31] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:6382: Connection reset by peer

[2025-12-31 10:49:42 DP4 TP16 EP16] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:64171: Connection reset by peer

[2025-12-31 10:49:42 DP5 TP21 EP21] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:6382: Connection reset by peer

[2025-12-31 10:49:42 DP5 TP20 EP20] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:55458: Connection reset by peer

[2025-12-31 10:49:42 DP7 TP28 EP28] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:20192: Connection reset by peer

[2025-12-31 10:49:42 DP6 TP25 EP25] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:62111: Connection reset by peer

[2025-12-31 10:49:42 DP5 TP23 EP23] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:33511: Connection reset by peer

[2025-12-31 10:49:42 DP7 TP29 EP29] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:19664: Connection reset by peer

[2025-12-31 10:49:42 DP4 TP19 EP19] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:20625: Connection reset by peer

[2025-12-31 10:49:42 DP4 TP18 EP18] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:36014: Connection reset by peer

[2025-12-31 10:49:42 DP5 TP22 EP22] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:20690: Connection reset by peer

[2025-12-31 10:49:42 DP6 TP26 EP26] Scheduler hit an exception: Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 2959, in run_scheduler_process
    scheduler.event_loop_overlap_disagg_decode()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/disaggregation/decode.py", line 861, in event_loop_overlap_disagg_decode
    recv_reqs = self.recv_requests()
                ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/scheduler.py", line 1292, in recv_requests
    control_reqs = broadcast_pyobj(
                   ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1214, in broadcast_pyobj
    dist.broadcast(tensor_size, src=src, group=dist_group)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2830, in broadcast
    work.wait()
RuntimeError: [/pytorch/third_party/gloo/gloo/transport/tcp/pair.cc:535] Read error [192.168.0.184]:36031: Connection reset by peer

Finished test case test/nightly/ascend/performance/test_ascend_deepseek_v3.2_w8a8_1p1d_32p_in64k_out1k_30ms.py
