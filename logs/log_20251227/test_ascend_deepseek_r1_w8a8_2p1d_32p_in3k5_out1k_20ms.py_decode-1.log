Writing to /root/.config/pip/pip.conf
Writing to /root/.config/pip/pip.conf
Writing to /root/.config/pip/pip.conf
Looking in indexes: http://cache-service.nginx-pypi-cache.svc.cluster.local/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Collecting kubernetes
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ca/ec/65f7d563aa4a62dd58777e8f6aa882f15db53b14eb29aba0c28a20f7eb26/kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 16.0 MB/s  0:00:00
Requirement already satisfied: certifi>=14.05.14 in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (2025.11.12)
Requirement already satisfied: six>=1.9.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (1.17.0)
Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (2.9.0.post0)
Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (6.0.3)
Collecting google-auth>=1.0.1 (from kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c6/97/451d55e05487a5cd6279a01a7e34921858b16f7dc8aa38a2c684743cd2b3/google_auth-2.45.0-py2.py3-none-any.whl (233 kB)
Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/34/db/b10e48aa8fff7407e67470363eac595018441cf32d5e1001567a7aeba5d2/websocket_client-1.9.0-py3-none-any.whl (82 kB)
Requirement already satisfied: requests in /usr/local/python3.11.13/lib/python3.11/site-packages (from kubernetes) (2.32.5)
Collecting requests-oauthlib (from kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)
Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c8/19/4ec628951a74043532ca2cf5d97b7b14863931476d117c471e8e2b1eb39f/urllib3-2.3.0-py3-none-any.whl (128 kB)
Collecting durationpy>=0.7 (from kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b0/0d/9feae160378a3553fa9a339b0e9c1a048e147a4127210e286ef18b730f03/durationpy-0.10-py3-none-any.whl (3.9 kB)
Collecting cachetools<7.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2c/fc/1d7b80d0eb7b714984ce40efc78859c022cd930e402f599d8ca9e39c78a4/cachetools-6.2.4-py3-none-any.whl (11 kB)
Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)
Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl (34 kB)
Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl (83 kB)
Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->kubernetes) (3.4.4)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->kubernetes) (3.11)
Collecting oauthlib>=3.0.0 (from requests-oauthlib->kubernetes)
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/be/9c/92789c596b8df838baa98fa71844d84283302f7604ed565dafe5a6b5041a/oauthlib-3.3.1-py3-none-any.whl (160 kB)
Installing collected packages: durationpy, websocket-client, urllib3, pyasn1, oauthlib, cachetools, rsa, pyasn1-modules, requests-oauthlib, google-auth, kubernetes
  Attempting uninstall: urllib3
    Found existing installation: urllib3 2.5.0
    Uninstalling urllib3-2.5.0:
      Successfully uninstalled urllib3-2.5.0

Successfully installed cachetools-6.2.4 durationpy-0.10 google-auth-2.45.0 kubernetes-34.1.0 oauthlib-3.3.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0 rsa-4.9.1 urllib3-2.3.0 websocket-client-1.9.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
Looking in indexes: http://cache-service.nginx-pypi-cache.svc.cluster.local/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Collecting xgrammar==0.1.25
  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/55/04/55a87e814bcab771d3e4159281fa382b3d5f14a36114f2f9e572728da831/xgrammar-0.1.25-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (8.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 42.2 MB/s  0:00:00
Requirement already satisfied: pydantic in /usr/local/python3.11.13/lib/python3.11/site-packages (from xgrammar==0.1.25) (2.12.5)
Requirement already satisfied: torch>=1.10.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from xgrammar==0.1.25) (2.8.0+cpu)
Requirement already satisfied: transformers>=4.38.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from xgrammar==0.1.25) (4.57.1)
Requirement already satisfied: ninja in /usr/local/python3.11.13/lib/python3.11/site-packages (from xgrammar==0.1.25) (1.13.0)
Requirement already satisfied: numpy in /usr/local/python3.11.13/lib/python3.11/site-packages (from xgrammar==0.1.25) (2.4.0)
Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from xgrammar==0.1.25) (4.15.0)
Requirement already satisfied: filelock in /usr/local/python3.11.13/lib/python3.11/site-packages (from torch>=1.10.0->xgrammar==0.1.25) (3.20.0)
Requirement already satisfied: sympy>=1.13.3 in /usr/local/python3.11.13/lib/python3.11/site-packages (from torch>=1.10.0->xgrammar==0.1.25) (1.14.0)
Requirement already satisfied: networkx in /usr/local/python3.11.13/lib/python3.11/site-packages (from torch>=1.10.0->xgrammar==0.1.25) (3.6.1)
Requirement already satisfied: jinja2 in /usr/local/python3.11.13/lib/python3.11/site-packages (from torch>=1.10.0->xgrammar==0.1.25) (3.1.6)
Requirement already satisfied: fsspec in /usr/local/python3.11.13/lib/python3.11/site-packages (from torch>=1.10.0->xgrammar==0.1.25) (2025.10.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.10.0->xgrammar==0.1.25) (1.3.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers>=4.38.0->xgrammar==0.1.25) (0.36.0)
Requirement already satisfied: packaging>=20.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers>=4.38.0->xgrammar==0.1.25) (25.0)
Requirement already satisfied: pyyaml>=5.1 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers>=4.38.0->xgrammar==0.1.25) (6.0.3)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers>=4.38.0->xgrammar==0.1.25) (2025.11.3)
Requirement already satisfied: requests in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers>=4.38.0->xgrammar==0.1.25) (2.32.5)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers>=4.38.0->xgrammar==0.1.25) (0.22.1)
Requirement already satisfied: safetensors>=0.4.3 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers>=4.38.0->xgrammar==0.1.25) (0.7.0)
Requirement already satisfied: tqdm>=4.27 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers>=4.38.0->xgrammar==0.1.25) (4.67.1)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/python3.11.13/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.38.0->xgrammar==0.1.25) (1.2.0)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->xgrammar==0.1.25) (2.1.5)
Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from pydantic->xgrammar==0.1.25) (0.7.0)
Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/python3.11.13/lib/python3.11/site-packages (from pydantic->xgrammar==0.1.25) (2.41.5)
Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python3.11.13/lib/python3.11/site-packages (from pydantic->xgrammar==0.1.25) (0.4.2)
Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->transformers>=4.38.0->xgrammar==0.1.25) (3.4.4)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->transformers>=4.38.0->xgrammar==0.1.25) (3.11)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->transformers>=4.38.0->xgrammar==0.1.25) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->transformers>=4.38.0->xgrammar==0.1.25) (2025.11.12)
Installing collected packages: xgrammar
  Attempting uninstall: xgrammar
    Found existing installation: xgrammar 0.1.27
    Uninstalling xgrammar-0.1.27:
      Successfully uninstalled xgrammar-0.1.27
Successfully installed xgrammar-0.1.25
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
Looking in indexes: http://cache-service.nginx-pypi-cache.svc.cluster.local/pypi/simple, https://pypi.tuna.tsinghua.edu.cn/simple
Requirement already satisfied: transformers==4.57.1 in /usr/local/python3.11.13/lib/python3.11/site-packages (4.57.1)
Requirement already satisfied: filelock in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (3.20.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (0.36.0)
Requirement already satisfied: numpy>=1.17 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (2.4.0)
Requirement already satisfied: packaging>=20.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (25.0)
Requirement already satisfied: pyyaml>=5.1 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (6.0.3)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (2025.11.3)
Requirement already satisfied: requests in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (2.32.5)
Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (0.22.1)
Requirement already satisfied: safetensors>=0.4.3 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (0.7.0)
Requirement already satisfied: tqdm>=4.27 in /usr/local/python3.11.13/lib/python3.11/site-packages (from transformers==4.57.1) (4.67.1)
Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/python3.11.13/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (2025.10.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/python3.11.13/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (4.15.0)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/python3.11.13/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1) (1.2.0)
Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->transformers==4.57.1) (3.4.4)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->transformers==4.57.1) (3.11)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->transformers==4.57.1) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/python3.11.13/lib/python3.11/site-packages (from requests->transformers==4.57.1) (2025.11.12)
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  732k  100  732k    0     0  1492k      0 --:--:-- --:--:-- --:--:-- 1494k
performance
vm.swappiness = 0
kernel.numa_balancing = 0
kernel.sched_migration_cost_ns = 50000
Running test case test/nightly/ascend/performance/test_ascend_deepseek_r1_w8a8_2p1d_32p_in3k5_out1k_20ms.py
The nic name matched is enp23s0f3
The nic name matched is enp23s0f3
Nic name: enp23s0f3
Init 192.168.0.102 cls.role=None!
[CI Test Method] Test_DeepSeek_R1_W8A8_2P1D_In3500_Out1024.test_throughput
launch_node start ......
query_configmap successfully!
monitor configmap.data={'sglang-multi-debug-sglang-decode-0': '192.168.0.60', 'sglang-multi-debug-sglang-decode-1': '192.168.0.102', 'sglang-multi-debug-sglang-prefill-0': '192.168.0.234', 'sglang-multi-debug-sglang-prefill-1': '192.168.0.184', 'sglang-multi-debug-sglang-router-0': '192.168.0.184'}
launch_node dist_init_addr='192.168.0.60:5000'
launch_node mf_addr='tcp://192.168.0.234:24666'
ENV_VAR SGLANG_SET_CPU_AFFINITY=1
ENV_VAR PYTORCH_NPU_ALLOC_CONF=expandable_segments:True
ENV_VAR STREAMS_PER_DEVICE=32
ENV_VAR SGLANG_NPU_USE_MLAPO=1
ENV_VAR SGLANG_USE_FIA_NZ=1
ENV_VAR SGLANG_ENABLE_OVERLAP_PLAN_STREAM=1
ENV_VAR SGLANG_ENABLE_SPEC_V2=1
ENV_VAR HCCL_BUFFSIZE=650
ENV_VAR SGLANG_DP_ROUND_ROBIN=1
ENV_VAR SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=12
ENV_VAR TASK_QUEUE_ENABLE=0
ENV_VAR SGLANG_SCHEDULER_SKIP_ALL_GATHER=1
ENV_VAR HCCL_SOCKET_IFNAME=enp23s0f3
ENV_VAR GLOO_SOCKET_IFNAME=enp23s0f3
Starting node, node_ip='192.168.0.102' service_args=['--trust-remote-code', '--attention-backend', 'ascend', '--device', 'npu', '--disaggregation-transfer-backend', 'ascend', '--nnodes', '2', '--disaggregation-mode', 'decode', '--tp-size', 32, '--dp-size', 16, '--mem-fraction-static', 0.75, '--max-running-requests', 32, '--quantization', 'modelslim', '--moe-a2a-backend', 'deepep', '--enable-dp-attention', '--deepep-mode', 'low_latency', '--enable-dp-lm-head', '--moe-dense-tp', '1', '--cuda-graph-bs', 2, 4, 6, '--watchdog-timeout', 9000, '--context-length', 8192, '--speculative-algorithm', 'NEXTN', '--speculative-num-steps', 3, '--speculative-eagle-topk', 1, '--speculative-num-draft-tokens', 4, '--tokenizer-worker-num', 4, '--prefill-round-robin-balance', '--disable-shared-experts-fusion', '--dtype', 'bfloat16', '--load-balance-method', 'decode_round_robin', '--dist-init-addr', '192.168.0.60:5000', '--node-rank', 1]
command=python3 -m sglang.launch_server --model-path /root/.cache/modelscope/hub/models/Howeee/DeepSeek-R1-0528-w8a8 --trust-remote-code --attention-backend ascend --device npu --disaggregation-transfer-backend ascend --nnodes 2 --disaggregation-mode decode --tp-size 32 --dp-size 16 --mem-fraction-static 0.75 --max-running-requests 32 --quantization modelslim --moe-a2a-backend deepep --enable-dp-attention --deepep-mode low_latency --enable-dp-lm-head --moe-dense-tp 1 --cuda-graph-bs 2 4 6 --watchdog-timeout 9000 --context-length 8192 --speculative-algorithm NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --tokenizer-worker-num 4 --prefill-round-robin-balance --disable-shared-experts-fusion --dtype bfloat16 --load-balance-method decode_round_robin --dist-init-addr 192.168.0.60:5000 --node-rank 1 --device npu --host 192.168.0.102 --port 8000
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-25 13:03:36] WARNING model_config.py:813: modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:36] WARNING server_args.py:1771: DP attention is enabled. The chunked prefill size is adjusted to 512 to avoid MoE kernel issues. 
[2025-12-25 13:03:36] WARNING server_args.py:1824: DeepEP MoE is enabled. The expert parallel size is adjusted to be the same as the tensor parallel size[32].
[2025-12-25 13:03:36] WARNING server_args.py:1966: Beta spec is enabled for eagle speculative decoding and overlap schedule is turned on.
[2025-12-25 13:03:36] WARNING server_args.py:2140: KV cache is forced as chunk cache for decode server
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:36] server_args=ServerArgs(model_path='/root/.cache/modelscope/hub/models/Howeee/DeepSeek-R1-0528-w8a8', tokenizer_path='/root/.cache/modelscope/hub/models/Howeee/DeepSeek-R1-0528-w8a8', tokenizer_mode='auto', tokenizer_worker_num=4, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=8192, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='192.168.0.102', port=8000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='bfloat16', quantization='modelslim', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.75, max_running_requests=32, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=512, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=32, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=198184593, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=9000.0, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/root/.cache/modelscope/hub/models/Howeee/DeepSeek-R1-0528-w8a8', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=16, load_balance_method='decode_round_robin', load_watch_interval=0.1, prefill_round_robin_balance=True, dist_init_addr='192.168.0.60:5000', nnodes=2, node_rank=1, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm='EAGLE', speculative_draft_model_path='/root/.cache/modelscope/hub/models/Howeee/DeepSeek-R1-0528-w8a8', speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=3, speculative_eagle_topk=1, speculative_num_draft_tokens=4, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization='modelslim', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, enable_mtp=False, ep_size=32, moe_a2a_backend='deepep', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='low_latency', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=1, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=True, cuda_graph_max_bs=6, cuda_graph_bs=[2, 4, 6], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=True, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=True, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='decode', disaggregation_transfer_backend='ascend', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2025-12-25 13:03:54 DP8 TP16 EP16] Process 1437 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
[2025-12-25 13:03:54 DP8 TP16 EP16] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:54 DP10 TP21 EP21] Process 1442 gpu_id 5 is running on CPUs: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]
[2025-12-25 13:03:54 DP12 TP25 EP25] Process 1446 gpu_id 9 is running on CPUs: [180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]
[2025-12-25 13:03:54 DP10 TP21 EP21] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:54 DP12 TP25 EP25] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
[2025-12-25 13:03:54 DP13 TP26 EP26] Process 1447 gpu_id 10 is running on CPUs: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219]
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:54 DP13 TP26 EP26] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:55 DP14 TP29 EP29] Process 1450 gpu_id 13 is running on CPUs: [260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279]
[2025-12-25 13:03:55 DP14 TP29 EP29] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-25 13:03:55 DP11 TP23 EP23] Process 1444 gpu_id 7 is running on CPUs: [140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159]
[2025-12-25 13:03:55 DP8 TP16 EP16] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:55 DP8 TP16 EP16] Init torch distributed begin.
[2025-12-25 13:03:55 DP11 TP23 EP23] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:55 DP10 TP21 EP21] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:55 DP10 TP21 EP21] Init torch distributed begin.
[2025-12-25 13:03:55 DP14 TP28 EP28] Process 1449 gpu_id 12 is running on CPUs: [240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259]
[2025-12-25 13:03:55 DP14 TP28 EP28] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:55 DP12 TP25 EP25] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:55 DP12 TP25 EP25] Init torch distributed begin.
[2025-12-25 13:03:55 DP12 TP24 EP24] Process 1445 gpu_id 8 is running on CPUs: [160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179]
[2025-12-25 13:03:55 DP12 TP24 EP24] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:55 DP15 TP31 EP31] Process 1452 gpu_id 15 is running on CPUs: [300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319]
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-25 13:03:55 DP15 TP31 EP31] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-25 13:03:55 DP9 TP19 EP19] Process 1440 gpu_id 3 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:55 DP9 TP19 EP19] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-25 13:03:55 DP13 TP26 EP26] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:55 DP13 TP26 EP26] Init torch distributed begin.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:55 DP13 TP27 EP27] Process 1448 gpu_id 11 is running on CPUs: [220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239]
[2025-12-25 13:03:55 DP13 TP27 EP27] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:56 DP15 TP30 EP30] Process 1451 gpu_id 14 is running on CPUs: [280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]
[2025-12-25 13:03:56 DP15 TP30 EP30] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP11 TP22 EP22] Process 1443 gpu_id 6 is running on CPUs: [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]
[2025-12-25 13:03:56 DP9 TP18 EP18] Process 1439 gpu_id 2 is running on CPUs: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:56 DP14 TP29 EP29] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP14 TP29 EP29] Init torch distributed begin.
[2025-12-25 13:03:56 DP11 TP22 EP22] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP9 TP18 EP18] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP10 TP20 EP20] Process 1441 gpu_id 4 is running on CPUs: [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
[2025-12-25 13:03:56 DP10 TP20 EP20] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP8 TP17 EP17] Process 1438 gpu_id 1 is running on CPUs: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2025-12-25 13:03:56 DP8 TP17 EP17] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2025-12-25 13:03:56 DP11 TP23 EP23] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP11 TP23 EP23] Init torch distributed begin.
[2025-12-25 13:03:56 DP14 TP28 EP28] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP14 TP28 EP28] Init torch distributed begin.
[2025-12-25 13:03:56 DP12 TP24 EP24] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP12 TP24 EP24] Init torch distributed begin.
[2025-12-25 13:03:56 DP15 TP31 EP31] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP15 TP31 EP31] Init torch distributed begin.
[2025-12-25 13:03:56 DP9 TP19 EP19] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP9 TP19 EP19] Init torch distributed begin.
[2025-12-25 13:03:56 DP13 TP27 EP27] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP13 TP27 EP27] Init torch distributed begin.
[2025-12-25 13:03:56 DP11 TP22 EP22] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP11 TP22 EP22] Init torch distributed begin.
[2025-12-25 13:03:56 DP10 TP20 EP20] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP10 TP20 EP20] Init torch distributed begin.
[2025-12-25 13:03:56 DP15 TP30 EP30] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP15 TP30 EP30] Init torch distributed begin.
[2025-12-25 13:03:56 DP9 TP18 EP18] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP9 TP18 EP18] Init torch distributed begin.
[2025-12-25 13:03:56 DP8 TP17 EP17] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:03:56 DP8 TP17 EP17] Init torch distributed begin.
[Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 17 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 18 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 16 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 19 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 21 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 22 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 23 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 25 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 26 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 27 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 29 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 30 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 31 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 28 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 20 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 24 is connected to 31 peer ranks. Expected number of connected peer ranks is : 31
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-12-25 13:03:58 DP15 TP30 EP30] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP15 TP31 EP31] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP14 TP28 EP28] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP14 TP29 EP29] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP13 TP27 EP27] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP13 TP26 EP26] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP12 TP25 EP25] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP11 TP23 EP23] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP12 TP24 EP24] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP11 TP22 EP22] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP10 TP21 EP21] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP10 TP20 EP20] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP9 TP18 EP18] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP9 TP19 EP19] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP8 TP17 EP17] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:03:58 DP8 TP16 EP16] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:03:59 DP11 TP22 EP22] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP11 TP23 EP23] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP12 TP25 EP25] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP8 TP16 EP16] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP14 TP29 EP29] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP14 TP28 EP28] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP10 TP20 EP20] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP10 TP21 EP21] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP9 TP19 EP19] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP12 TP24 EP24] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP15 TP31 EP31] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP15 TP30 EP30] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP8 TP17 EP17] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP9 TP18 EP18] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP13 TP26 EP26] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP13 TP27 EP27] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-25 13:03:59 DP10 TP20 EP20] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP13 TP27 EP27] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP8 TP17 EP17] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP8 TP16 EP16] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP12 TP25 EP25] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP15 TP31 EP31] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP10 TP21 EP21] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP9 TP18 EP18] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP15 TP30 EP30] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP14 TP29 EP29] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP11 TP23 EP23] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP11 TP22 EP22] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP14 TP28 EP28] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP12 TP24 EP24] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP9 TP19 EP19] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
[2025-12-25 13:03:59 DP13 TP26 EP26] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-25 13:04:01 DP11 TP23 EP23] Load weight begin. avail mem=61.08 GB
[2025-12-25 13:04:01 DP11 TP22 EP22] Load weight begin. avail mem=60.85 GB
[2025-12-25 13:04:01 DP10 TP20 EP20] Load weight begin. avail mem=60.85 GB
[2025-12-25 13:04:01 DP10 TP21 EP21] Load weight begin. avail mem=61.08 GB
[2025-12-25 13:04:01 DP13 TP27 EP27] Load weight begin. avail mem=61.08 GB
[2025-12-25 13:04:01 DP8 TP16 EP16] Load weight begin. avail mem=60.79 GB
[2025-12-25 13:04:01 DP14 TP29 EP29] Load weight begin. avail mem=61.08 GB
[2025-12-25 13:04:01 DP8 TP17 EP17] Load weight begin. avail mem=61.08 GB
[2025-12-25 13:04:01 DP15 TP31 EP31] Load weight begin. avail mem=61.08 GB
[2025-12-25 13:04:01 DP14 TP28 EP28] Load weight begin. avail mem=60.85 GB
[2025-12-25 13:04:01 DP15 TP30 EP30] Load weight begin. avail mem=60.86 GB
[2025-12-25 13:04:01 DP12 TP25 EP25] Load weight begin. avail mem=61.09 GB
[2025-12-25 13:04:01 DP12 TP24 EP24] Load weight begin. avail mem=60.85 GB
[2025-12-25 13:04:01 DP9 TP19 EP19] Load weight begin. avail mem=61.08 GB
[2025-12-25 13:04:01 DP13 TP26 EP26] Load weight begin. avail mem=60.86 GB
[2025-12-25 13:04:01 DP9 TP18 EP18] Load weight begin. avail mem=60.85 GB
[2025-12-25 13:11:03 DP11 TP22 EP22] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.01 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP13 TP27 EP27] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.24 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP8 TP16 EP16] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=26.95 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP9 TP19 EP19] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.24 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP15 TP30 EP30] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.01 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP12 TP25 EP25] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.24 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP14 TP28 EP28] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.01 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP10 TP21 EP21] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.24 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP11 TP23 EP23] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.24 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP15 TP31 EP31] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.24 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP13 TP26 EP26] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.01 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP8 TP17 EP17] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.23 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP14 TP29 EP29] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.24 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP12 TP24 EP24] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.01 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP10 TP20 EP20] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.01 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP9 TP18 EP18] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=27.00 GB, mem usage=33.84 GB.
[2025-12-25 13:11:04 DP15 TP31 EP31] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP15 TP30 EP30] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP14 TP29 EP29] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP14 TP28 EP28] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP13 TP27 EP27] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP13 TP26 EP26] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP12 TP25 EP25] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP12 TP24 EP24] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP11 TP23 EP23] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP11 TP22 EP22] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP10 TP21 EP21] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP10 TP20 EP20] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP9 TP19 EP19] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP9 TP18 EP18] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP8 TP17 EP17] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP8 TP16 EP16] The available memory for KV cache is 11.73 GB.
[2025-12-25 13:11:04 DP10 TP21 EP21] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP11 TP23 EP23] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP8 TP16 EP16] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP8 TP17 EP17] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP9 TP19 EP19] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP9 TP18 EP18] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP15 TP30 EP30] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP11 TP22 EP22] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP14 TP28 EP28] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP10 TP20 EP20] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP13 TP27 EP27] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP15 TP31 EP31] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP13 TP26 EP26] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP14 TP29 EP29] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP12 TP24 EP24] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP12 TP25 EP25] KV Cache is allocated. #tokens: 179584, KV size: 11.76 GB
[2025-12-25 13:11:04 DP11 TP23 EP23] Memory pool end. avail mem=15.48 GB
[2025-12-25 13:11:04 DP9 TP18 EP18] Memory pool end. avail mem=15.25 GB
[2025-12-25 13:11:04 DP13 TP26 EP26] Memory pool end. avail mem=15.26 GB
[2025-12-25 13:11:04 DP8 TP16 EP16] Memory pool end. avail mem=15.19 GB
[2025-12-25 13:11:04 DP12 TP24 EP24] Memory pool end. avail mem=15.25 GB
[2025-12-25 13:11:04 DP15 TP30 EP30] Memory pool end. avail mem=15.26 GB
[2025-12-25 13:11:04 DP8 TP17 EP17] Memory pool end. avail mem=15.48 GB
[2025-12-25 13:11:04 DP14 TP28 EP28] Memory pool end. avail mem=15.25 GB
[2025-12-25 13:11:04 DP10 TP20 EP20] Memory pool end. avail mem=15.25 GB
[2025-12-25 13:11:04 DP10 TP21 EP21] Memory pool end. avail mem=15.48 GB
[2025-12-25 13:11:04 DP9 TP19 EP19] Memory pool end. avail mem=15.48 GB
[2025-12-25 13:11:04 DP13 TP27 EP27] Memory pool end. avail mem=15.48 GB
[2025-12-25 13:11:04 DP12 TP25 EP25] Memory pool end. avail mem=15.49 GB
[2025-12-25 13:11:04 DP15 TP31 EP31] Memory pool end. avail mem=15.48 GB
[2025-12-25 13:11:04 DP14 TP29 EP29] Memory pool end. avail mem=15.48 GB
[2025-12-25 13:11:04 DP11 TP22 EP22] Memory pool end. avail mem=15.25 GB
[2025-12-25 13:11:04 DP11 TP23 EP23] Capture cuda graph begin. This can take up to several minutes. avail mem=15.48 GB
[2025-12-25 13:11:04 DP14 TP28 EP28] Capture cuda graph begin. This can take up to several minutes. avail mem=15.25 GB
[2025-12-25 13:11:04 DP15 TP30 EP30] Capture cuda graph begin. This can take up to several minutes. avail mem=15.26 GB
[2025-12-25 13:11:04 DP10 TP20 EP20] Capture cuda graph begin. This can take up to several minutes. avail mem=15.25 GB
[2025-12-25 13:11:04 DP14 TP29 EP29] Capture cuda graph begin. This can take up to several minutes. avail mem=15.48 GB
[2025-12-25 13:11:04 DP8 TP16 EP16] Capture cuda graph begin. This can take up to several minutes. avail mem=15.19 GB
[2025-12-25 13:11:04 DP11 TP22 EP22] Capture cuda graph begin. This can take up to several minutes. avail mem=15.25 GB
[2025-12-25 13:11:04 DP8 TP17 EP17] Capture cuda graph begin. This can take up to several minutes. avail mem=15.48 GB
[2025-12-25 13:11:04 DP12 TP25 EP25] Capture cuda graph begin. This can take up to several minutes. avail mem=15.49 GB
[2025-12-25 13:11:04 DP10 TP21 EP21] Capture cuda graph begin. This can take up to several minutes. avail mem=15.48 GB
[2025-12-25 13:11:04 DP13 TP27 EP27] Capture cuda graph begin. This can take up to several minutes. avail mem=15.48 GB
[2025-12-25 13:11:04 DP12 TP24 EP24] Capture cuda graph begin. This can take up to several minutes. avail mem=15.25 GB
[2025-12-25 13:11:04 DP9 TP18 EP18] Capture cuda graph begin. This can take up to several minutes. avail mem=15.25 GB
[2025-12-25 13:11:04 DP9 TP19 EP19] Capture cuda graph begin. This can take up to several minutes. avail mem=15.48 GB
[2025-12-25 13:11:04 DP15 TP31 EP31] Capture cuda graph begin. This can take up to several minutes. avail mem=15.48 GB
[2025-12-25 13:11:04 DP13 TP26 EP26] Capture cuda graph begin. This can take up to several minutes. avail mem=15.26 GB
[rank18]:[W1225 13:11:10.047570149 compiler_depend.ts:198] Warning: Driver Version: "�Ԛ��" is invalid or not supported yet. (function operator())
[rank19]:[W1225 13:11:10.047570219 compiler_depend.ts:198] Warning: Driver Version: "��{��" is invalid or not supported yet. (function operator())
[rank21]:[W1225 13:11:10.047579129 compiler_depend.ts:198] Warning: Driver Version: "�l���" is invalid or not supported yet. (function operator())
[rank20]:[W1225 13:11:10.047578969 compiler_depend.ts:198] Warning: Driver Version: "�����" is invalid or not supported yet. (function operator())
[rank22]:[W1225 13:11:10.047579019 compiler_depend.ts:198] Warning: Driver Version: "�*���" is invalid or not supported yet. (function operator())
[rank23]:[W1225 13:11:10.047578949 compiler_depend.ts:198] Warning: Driver Version: "�>���" is invalid or not supported yet. (function operator())
[rank16]:[W1225 13:11:10.050813444 compiler_depend.ts:198] Warning: Driver Version: "�6���" is invalid or not supported yet. (function operator())
[rank17]:[W1225 13:11:10.050901054 compiler_depend.ts:198] Warning: Driver Version: "�|���" is invalid or not supported yet. (function operator())
[rank26]:[W1225 13:11:10.090387521 compiler_depend.ts:198] Warning: Driver Version: "�����" is invalid or not supported yet. (function operator())
[rank27]:[W1225 13:11:10.090387061 compiler_depend.ts:198] Warning: Driver Version: "�9���" is invalid or not supported yet. (function operator())
[rank24]:[W1225 13:11:10.094679131 compiler_depend.ts:198] Warning: Driver Version: "�����" is invalid or not supported yet. (function operator())
[rank25]:[W1225 13:11:10.094682491 compiler_depend.ts:198] Warning: Driver Version: "����" is invalid or not supported yet. (function operator())
[rank29]:[W1225 13:11:10.110588646 compiler_depend.ts:198] Warning: Driver Version: "����" is invalid or not supported yet. (function operator())
[rank28]:[W1225 13:11:10.110604117 compiler_depend.ts:198] Warning: Driver Version: "��~��" is invalid or not supported yet. (function operator())
[rank30]:[W1225 13:11:10.136208777 compiler_depend.ts:198] Warning: Driver Version: "����" is invalid or not supported yet. (function operator())
[rank31]:[W1225 13:11:10.136458339 compiler_depend.ts:198] Warning: Driver Version: "��|��" is invalid or not supported yet. (function operator())
[rank18]:[W1225 13:11:10.366465845 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank19]:[W1225 13:11:10.366466205 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank23]:[W1225 13:11:10.379884309 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank20]:[W1225 13:11:10.380946814 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank21]:[W1225 13:11:10.380979854 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank22]:[W1225 13:11:10.382265150 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank16]:[W1225 13:11:10.385205494 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank17]:[W1225 13:11:10.385205264 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank26]:[W1225 13:11:10.390634930 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank27]:[W1225 13:11:10.390634720 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank24]:[W1225 13:11:10.396420917 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank25]:[W1225 13:11:10.396421257 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank29]:[W1225 13:11:10.406015952 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank28]:[W1225 13:11:10.406779016 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank31]:[W1225 13:11:10.469858704 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank30]:[W1225 13:11:10.469858524 compiler_depend.ts:83] Warning: [Check][offset] Check input storage_offset[%ld] = 0 failed, result is untrustworthy128 (function operator())
[rank23]:[W1225 13:11:30.547658866 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank21]:[W1225 13:11:30.547660706 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank17]:[W1225 13:11:30.547699876 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank22]:[W1225 13:11:30.547708097 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank16]:[W1225 13:11:30.547709767 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank20]:[W1225 13:11:30.547712407 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank18]:[W1225 13:11:30.547734177 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank19]:[W1225 13:11:30.547734247 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank29]:[W1225 13:11:30.547734067 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank28]:[W1225 13:11:30.547734267 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank30]:[W1225 13:11:30.547740117 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank25]:[W1225 13:11:30.547740197 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank31]:[W1225 13:11:30.547737537 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank26]:[W1225 13:11:30.547752757 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank27]:[W1225 13:11:30.547750427 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank24]:[W1225 13:11:30.547773737 compiler_depend.ts:207] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[2025-12-25 13:11:36 DP10 TP21 EP21] Capture cuda graph end. Time elapsed: 32.28 s. mem usage=5.85 GB. avail mem=9.63 GB.
[2025-12-25 13:11:36 DP11 TP23 EP23] Capture cuda graph end. Time elapsed: 32.29 s. mem usage=5.85 GB. avail mem=9.63 GB.
[2025-12-25 13:11:36 DP10 TP20 EP20] Capture cuda graph end. Time elapsed: 32.30 s. mem usage=5.85 GB. avail mem=9.40 GB.
[2025-12-25 13:11:36 DP11 TP22 EP22] Capture cuda graph end. Time elapsed: 32.31 s. mem usage=5.86 GB. avail mem=9.40 GB.
[2025-12-25 13:11:36 DP8 TP16 EP16] Capture cuda graph end. Time elapsed: 32.40 s. mem usage=5.85 GB. avail mem=9.34 GB.
[2025-12-25 13:11:36 DP8 TP17 EP17] Capture cuda graph end. Time elapsed: 32.42 s. mem usage=5.85 GB. avail mem=9.62 GB.
[2025-12-25 13:11:36 DP13 TP27 EP27] Capture cuda graph end. Time elapsed: 32.45 s. mem usage=5.85 GB. avail mem=9.63 GB.
[2025-12-25 13:11:36 DP14 TP29 EP29] Capture cuda graph end. Time elapsed: 32.47 s. mem usage=5.85 GB. avail mem=9.63 GB.
[2025-12-25 13:11:36 DP15 TP31 EP31] Capture cuda graph end. Time elapsed: 32.47 s. mem usage=5.85 GB. avail mem=9.63 GB.
[2025-12-25 13:11:36 DP9 TP19 EP19] Capture cuda graph end. Time elapsed: 32.47 s. mem usage=5.85 GB. avail mem=9.63 GB.
[2025-12-25 13:11:36 DP14 TP28 EP28] Capture cuda graph end. Time elapsed: 32.47 s. mem usage=5.85 GB. avail mem=9.40 GB.
[2025-12-25 13:11:36 DP15 TP30 EP30] Capture cuda graph end. Time elapsed: 32.48 s. mem usage=5.85 GB. avail mem=9.40 GB.
[2025-12-25 13:11:36 DP12 TP25 EP25] Capture cuda graph end. Time elapsed: 32.48 s. mem usage=5.85 GB. avail mem=9.63 GB.
[2025-12-25 13:11:36 DP9 TP18 EP18] Capture cuda graph end. Time elapsed: 32.48 s. mem usage=5.85 GB. avail mem=9.39 GB.
[2025-12-25 13:11:36 DP12 TP24 EP24] Capture cuda graph end. Time elapsed: 32.48 s. mem usage=5.85 GB. avail mem=9.40 GB.
[2025-12-25 13:11:36 DP13 TP26 EP26] Capture cuda graph end. Time elapsed: 32.54 s. mem usage=5.85 GB. avail mem=9.40 GB.
[2025-12-25 13:11:37 DP10 TP21 EP21] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP10 TP20 EP20] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP8 TP17 EP17] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP8 TP16 EP16] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP10 TP21 EP21] Init torch distributed begin.
[2025-12-25 13:11:37 DP10 TP20 EP20] Init torch distributed begin.
[2025-12-25 13:11:37 DP8 TP16 EP16] Init torch distributed begin.
[2025-12-25 13:11:37 DP8 TP17 EP17] Init torch distributed begin.
[2025-12-25 13:11:37 DP14 TP29 EP29] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP14 TP29 EP29] Init torch distributed begin.
[2025-12-25 13:11:37 DP14 TP28 EP28] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP14 TP28 EP28] Init torch distributed begin.
[2025-12-25 13:11:37 DP12 TP25 EP25] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP12 TP25 EP25] Init torch distributed begin.
[2025-12-25 13:11:37 DP11 TP22 EP22] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP11 TP22 EP22] Init torch distributed begin.
[2025-12-25 13:11:37 DP12 TP24 EP24] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP12 TP24 EP24] Init torch distributed begin.
[2025-12-25 13:11:37 DP9 TP18 EP18] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP15 TP30 EP30] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP9 TP18 EP18] Init torch distributed begin.
[2025-12-25 13:11:37 DP15 TP30 EP30] Init torch distributed begin.
[2025-12-25 13:11:37 DP13 TP26 EP26] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP13 TP26 EP26] Init torch distributed begin.
[2025-12-25 13:11:37 DP11 TP23 EP23] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP11 TP23 EP23] Init torch distributed begin.
[2025-12-25 13:11:37 DP13 TP27 EP27] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP15 TP31 EP31] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP9 TP19 EP19] modelslim quantization is not fully optimized yet. The speed can be slower than non-quantized models.
[2025-12-25 13:11:37 DP13 TP27 EP27] Init torch distributed begin.
[2025-12-25 13:11:37 DP15 TP31 EP31] Init torch distributed begin.
[2025-12-25 13:11:37 DP9 TP19 EP19] Init torch distributed begin.
[2025-12-25 13:11:37 DP15 TP31 EP31] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP15 TP30 EP30] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP14 TP29 EP29] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP14 TP28 EP28] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP13 TP27 EP27] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP13 TP26 EP26] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP11 TP23 EP23] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP12 TP25 EP25] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP11 TP22 EP22] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP12 TP24 EP24] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP10 TP20 EP20] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP9 TP19 EP19] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP10 TP21 EP21] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP9 TP18 EP18] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP8 TP16 EP16] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP8 TP17 EP17] Init torch distributed ends. mem usage=0.00 GB
[2025-12-25 13:11:37 DP15 TP30 EP30] Load weight begin. avail mem=9.40 GB
[2025-12-25 13:11:37 DP15 TP31 EP31] Load weight begin. avail mem=9.63 GB
[2025-12-25 13:11:37 DP14 TP28 EP28] Load weight begin. avail mem=9.40 GB
[2025-12-25 13:11:37 DP14 TP29 EP29] Load weight begin. avail mem=9.63 GB
[2025-12-25 13:11:37 DP13 TP27 EP27] Load weight begin. avail mem=9.63 GB
[2025-12-25 13:11:37 DP11 TP22 EP22] Load weight begin. avail mem=9.40 GB
[2025-12-25 13:11:37 DP11 TP23 EP23] Load weight begin. avail mem=9.63 GB
[2025-12-25 13:11:37 DP12 TP24 EP24] Load weight begin. avail mem=9.40 GB
[2025-12-25 13:11:37 DP10 TP20 EP20] Load weight begin. avail mem=9.40 GB
[2025-12-25 13:11:37 DP12 TP25 EP25] Load weight begin. avail mem=9.63 GB
[2025-12-25 13:11:37 DP13 TP26 EP26] Load weight begin. avail mem=9.40 GB
[2025-12-25 13:11:37 DP9 TP18 EP18] Load weight begin. avail mem=9.39 GB
[2025-12-25 13:11:37 DP9 TP19 EP19] Load weight begin. avail mem=9.63 GB
[2025-12-25 13:11:37 DP10 TP21 EP21] Load weight begin. avail mem=9.63 GB
[2025-12-25 13:11:37 DP8 TP16 EP16] Load weight begin. avail mem=9.34 GB
[2025-12-25 13:11:37 DP8 TP17 EP17] Load weight begin. avail mem=9.62 GB
[2025-12-25 13:13:23 DP9 TP18 EP18] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.17 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP13 TP27 EP27] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.40 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP9 TP19 EP19] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.41 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP11 TP23 EP23] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.41 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP12 TP25 EP25] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.41 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP10 TP21 EP21] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.41 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP14 TP29 EP29] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.41 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP11 TP22 EP22] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.17 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP8 TP16 EP16] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.11 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP8 TP17 EP17] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.40 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP14 TP28 EP28] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.18 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP12 TP24 EP24] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.17 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP15 TP31 EP31] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.40 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP15 TP30 EP30] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.18 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP10 TP20 EP20] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.18 GB, mem usage=3.22 GB.
[2025-12-25 13:13:23 DP13 TP26 EP26] Load weight end. type=DeepseekV3ForCausalLMNextN, dtype=torch.bfloat16, avail mem=6.18 GB, mem usage=3.22 GB.
[2025-12-25 13:13:26 DP14 TP29 EP29] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP15 TP30 EP30] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP15 TP31 EP31] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP14 TP28 EP28] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP13 TP27 EP27] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP13 TP26 EP26] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP12 TP25 EP25] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP11 TP23 EP23] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP12 TP24 EP24] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP11 TP22 EP22] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP10 TP21 EP21] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP10 TP20 EP20] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP9 TP19 EP19] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP9 TP18 EP18] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP8 TP17 EP17] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP8 TP16 EP16] The available memory for KV cache is 3.77 GB.
[2025-12-25 13:13:26 DP14 TP28 EP28] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP14 TP29 EP29] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP11 TP22 EP22] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP11 TP23 EP23] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP10 TP21 EP21] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP13 TP27 EP27] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP13 TP26 EP26] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP15 TP30 EP30] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP15 TP31 EP31] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP10 TP20 EP20] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP12 TP25 EP25] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP12 TP24 EP24] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP9 TP19 EP19] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP8 TP17 EP17] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP8 TP16 EP16] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP9 TP18 EP18] KV Cache is allocated. #tokens: 179584, KV size: 0.19 GB
[2025-12-25 13:13:26 DP14 TP28 EP28] Memory pool end. avail mem=6.02 GB
[2025-12-25 13:13:26 DP14 TP29 EP29] Memory pool end. avail mem=6.25 GB
[2025-12-25 13:13:26 DP11 TP22 EP22] Memory pool end. avail mem=6.02 GB
[2025-12-25 13:13:26 DP11 TP23 EP23] Memory pool end. avail mem=6.25 GB
[2025-12-25 13:13:26 DP13 TP27 EP27] Memory pool end. avail mem=6.25 GB
[2025-12-25 13:13:26 DP15 TP30 EP30] Memory pool end. avail mem=6.02 GB
[2025-12-25 13:13:26 DP13 TP26 EP26] Memory pool end. avail mem=6.02 GB
[2025-12-25 13:13:26 DP10 TP20 EP20] Memory pool end. avail mem=6.02 GB
[2025-12-25 13:13:26 DP10 TP21 EP21] Memory pool end. avail mem=6.25 GB
[2025-12-25 13:13:26 DP15 TP31 EP31] Memory pool end. avail mem=6.25 GB
[2025-12-25 13:13:26 DP12 TP25 EP25] Memory pool end. avail mem=6.25 GB
[2025-12-25 13:13:26 DP12 TP24 EP24] Memory pool end. avail mem=6.02 GB
[2025-12-25 13:13:26 DP9 TP19 EP19] Memory pool end. avail mem=6.25 GB
[2025-12-25 13:13:26 DP9 TP18 EP18] Memory pool end. avail mem=6.01 GB
[2025-12-25 13:13:26 DP8 TP16 EP16] Memory pool end. avail mem=5.96 GB
[2025-12-25 13:13:26 DP8 TP17 EP17] Memory pool end. avail mem=6.24 GB
[2025-12-25 13:13:28 DP12 TP25 EP25] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.81 GB
[2025-12-25 13:13:28 DP9 TP19 EP19] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.81 GB
[2025-12-25 13:13:28 DP10 TP21 EP21] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.80 GB
[2025-12-25 13:13:28 DP13 TP27 EP27] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.80 GB
[2025-12-25 13:13:28 DP11 TP22 EP22] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.57 GB
[2025-12-25 13:13:28 DP11 TP23 EP23] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.81 GB
[2025-12-25 13:13:28 DP14 TP29 EP29] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.80 GB
[2025-12-25 13:13:28 DP14 TP28 EP28] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.58 GB
[2025-12-25 13:13:28 DP10 TP20 EP20] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.57 GB
[2025-12-25 13:13:28 DP15 TP30 EP30] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.58 GB
[2025-12-25 13:13:28 DP15 TP31 EP31] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.80 GB
[2025-12-25 13:13:28 DP8 TP16 EP16] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.51 GB
[2025-12-25 13:13:28 DP8 TP17 EP17] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.80 GB
[2025-12-25 13:13:28 DP12 TP24 EP24] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.57 GB
[2025-12-25 13:13:28 DP9 TP18 EP18] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.57 GB
[2025-12-25 13:13:28 DP13 TP26 EP26] Capture draft cuda graph begin. This can take up to several minutes. avail mem=8.58 GB
[2025-12-25 13:13:35 DP11 TP23 EP23] Capture draft cuda graph end. Time elapsed: 7.04 s. mem usage=0.39 GB. avail mem=8.42 GB.
[2025-12-25 13:13:35 DP11 TP23 EP23] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.42 GB
[2025-12-25 13:13:35 DP11 TP22 EP22] Capture draft cuda graph end. Time elapsed: 7.04 s. mem usage=0.39 GB. avail mem=8.18 GB.
[2025-12-25 13:13:35 DP10 TP21 EP21] Capture draft cuda graph end. Time elapsed: 7.05 s. mem usage=0.39 GB. avail mem=8.41 GB.
[2025-12-25 13:13:35 DP11 TP22 EP22] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.18 GB
[2025-12-25 13:13:35 DP10 TP21 EP21] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.41 GB
[2025-12-25 13:13:35 DP10 TP20 EP20] Capture draft cuda graph end. Time elapsed: 7.08 s. mem usage=0.39 GB. avail mem=8.18 GB.
[2025-12-25 13:13:35 DP10 TP20 EP20] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.18 GB
[2025-12-25 13:13:35 DP13 TP27 EP27] Capture draft cuda graph end. Time elapsed: 7.15 s. mem usage=0.39 GB. avail mem=8.41 GB.
[2025-12-25 13:13:35 DP13 TP27 EP27] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.41 GB
[2025-12-25 13:13:35 DP8 TP16 EP16] Capture draft cuda graph end. Time elapsed: 7.16 s. mem usage=0.39 GB. avail mem=8.12 GB.
[2025-12-25 13:13:35 DP8 TP16 EP16] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.12 GB
[2025-12-25 13:13:35 DP14 TP29 EP29] Capture draft cuda graph end. Time elapsed: 7.17 s. mem usage=0.39 GB. avail mem=8.41 GB.
[2025-12-25 13:13:35 DP14 TP29 EP29] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.41 GB
[2025-12-25 13:13:35 DP14 TP28 EP28] Capture draft cuda graph end. Time elapsed: 7.17 s. mem usage=0.39 GB. avail mem=8.19 GB.
[2025-12-25 13:13:35 DP14 TP28 EP28] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.19 GB
[2025-12-25 13:13:35 DP15 TP31 EP31] Capture draft cuda graph end. Time elapsed: 7.18 s. mem usage=0.39 GB. avail mem=8.41 GB.
[2025-12-25 13:13:35 DP15 TP31 EP31] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.41 GB
[2025-12-25 13:13:35 DP15 TP30 EP30] Capture draft cuda graph end. Time elapsed: 7.18 s. mem usage=0.39 GB. avail mem=8.19 GB.
[2025-12-25 13:13:35 DP15 TP30 EP30] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.19 GB
[2025-12-25 13:13:35 DP8 TP17 EP17] Capture draft cuda graph end. Time elapsed: 7.18 s. mem usage=0.39 GB. avail mem=8.41 GB.
[2025-12-25 13:13:35 DP8 TP17 EP17] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.41 GB
[2025-12-25 13:13:35 DP12 TP25 EP25] Capture draft cuda graph end. Time elapsed: 7.21 s. mem usage=0.39 GB. avail mem=8.42 GB.
[2025-12-25 13:13:35 DP12 TP25 EP25] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.42 GB
[2025-12-25 13:13:35 DP12 TP24 EP24] Capture draft cuda graph end. Time elapsed: 7.19 s. mem usage=0.39 GB. avail mem=8.18 GB.
[2025-12-25 13:13:35 DP12 TP24 EP24] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.18 GB
[2025-12-25 13:13:35 DP9 TP19 EP19] Capture draft cuda graph end. Time elapsed: 7.23 s. mem usage=0.39 GB. avail mem=8.42 GB.
[2025-12-25 13:13:35 DP9 TP19 EP19] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.42 GB
[2025-12-25 13:13:35 DP9 TP18 EP18] Capture draft cuda graph end. Time elapsed: 7.21 s. mem usage=0.39 GB. avail mem=8.18 GB.
[2025-12-25 13:13:35 DP9 TP18 EP18] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.18 GB
[2025-12-25 13:13:35 DP13 TP26 EP26] Capture draft cuda graph end. Time elapsed: 7.21 s. mem usage=0.39 GB. avail mem=8.19 GB.
[2025-12-25 13:13:35 DP13 TP26 EP26] Capture draft extend cuda graph begin. This can take up to several minutes. avail mem=8.19 GB
[2025-12-25 13:13:39 DP10 TP21 EP21] Capture draft extend cuda graph end. Time elapsed: 3.95 s. mem usage=0.26 GB. avail mem=8.15 GB.
[2025-12-25 13:13:39 DP11 TP23 EP23] Capture draft extend cuda graph end. Time elapsed: 3.97 s. mem usage=0.26 GB. avail mem=8.16 GB.
[2025-12-25 13:13:39 DP11 TP22 EP22] Capture draft extend cuda graph end. Time elapsed: 3.97 s. mem usage=0.26 GB. avail mem=7.92 GB.
[2025-12-25 13:13:39 DP10 TP20 EP20] Capture draft extend cuda graph end. Time elapsed: 3.93 s. mem usage=0.26 GB. avail mem=7.92 GB.
[2025-12-25 13:13:39 DP13 TP27 EP27] Capture draft extend cuda graph end. Time elapsed: 3.95 s. mem usage=0.26 GB. avail mem=8.15 GB.
[2025-12-25 13:13:39 DP8 TP16 EP16] Capture draft extend cuda graph end. Time elapsed: 3.96 s. mem usage=0.26 GB. avail mem=7.86 GB.
[2025-12-25 13:13:39 DP14 TP29 EP29] Capture draft extend cuda graph end. Time elapsed: 3.95 s. mem usage=0.26 GB. avail mem=8.15 GB.
[2025-12-25 13:13:39 DP14 TP28 EP28] Capture draft extend cuda graph end. Time elapsed: 3.96 s. mem usage=0.26 GB. avail mem=7.92 GB.
[2025-12-25 13:13:39 DP15 TP30 EP30] Capture draft extend cuda graph end. Time elapsed: 3.95 s. mem usage=0.26 GB. avail mem=7.93 GB.
[2025-12-25 13:13:39 DP15 TP31 EP31] Capture draft extend cuda graph end. Time elapsed: 3.95 s. mem usage=0.26 GB. avail mem=8.15 GB.
[2025-12-25 13:13:39 DP8 TP17 EP17] Capture draft extend cuda graph end. Time elapsed: 3.95 s. mem usage=0.26 GB. avail mem=8.15 GB.
[2025-12-25 13:13:39 DP12 TP25 EP25] Capture draft extend cuda graph end. Time elapsed: 3.96 s. mem usage=0.26 GB. avail mem=8.16 GB.
[2025-12-25 13:13:39 DP12 TP24 EP24] Capture draft extend cuda graph end. Time elapsed: 3.95 s. mem usage=0.26 GB. avail mem=7.92 GB.
[2025-12-25 13:13:39 DP9 TP19 EP19] Capture draft extend cuda graph end. Time elapsed: 3.96 s. mem usage=0.26 GB. avail mem=8.16 GB.
[2025-12-25 13:13:39 DP9 TP18 EP18] Capture draft extend cuda graph end. Time elapsed: 3.95 s. mem usage=0.26 GB. avail mem=7.92 GB.
[2025-12-25 13:13:39 DP13 TP26 EP26] Capture draft extend cuda graph end. Time elapsed: 3.94 s. mem usage=0.26 GB. avail mem=7.93 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=196, family=2, type=2, proto=0, laddr=('192.168.0.102', 43093), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=214, family=2, type=2, proto=0, laddr=('192.168.0.102', 44984), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=196, family=2, type=2, proto=0, laddr=('192.168.0.102', 47365), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=213, family=2, type=2, proto=0, laddr=('192.168.0.102', 51926), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=203, family=2, type=2, proto=0, laddr=('192.168.0.102', 34359), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=203, family=2, type=2, proto=0, laddr=('192.168.0.102', 53566), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=222, family=2, type=2, proto=0, laddr=('192.168.0.102', 49212), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=222, family=2, type=2, proto=0, laddr=('192.168.0.102', 46832), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP11 TP23 EP23] Invalid or no transfer protocol specified, using default protocol.
[2025-12-25 13:13:39 DP11 TP22 EP22] Invalid or no transfer protocol specified, using default protocol.
[2025-12-25 13:13:39 DP10 TP21 EP21] Invalid or no transfer protocol specified, using default protocol.
[2025-12-25 13:13:39 DP10 TP20 EP20] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=196, family=2, type=2, proto=0, laddr=('192.168.0.102', 45212), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=203, family=2, type=2, proto=0, laddr=('192.168.0.102', 47306), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP13 TP27 EP27] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=213, family=2, type=2, proto=0, laddr=('192.168.0.102', 39387), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=222, family=2, type=2, proto=0, laddr=('192.168.0.102', 52859), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP8 TP16 EP16] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=213, family=2, type=2, proto=0, laddr=('192.168.0.102', 43851), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=196, family=2, type=2, proto=0, laddr=('192.168.0.102', 35814), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=196, family=2, type=2, proto=0, laddr=('192.168.0.102', 35349), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=214, family=2, type=2, proto=0, laddr=('192.168.0.102', 37499), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=222, family=2, type=2, proto=0, laddr=('192.168.0.102', 55054), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP14 TP28 EP28] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=203, family=2, type=2, proto=0, laddr=('192.168.0.102', 46380), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP14 TP29 EP29] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=203, family=2, type=2, proto=0, laddr=('192.168.0.102', 36991), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP15 TP31 EP31] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=222, family=2, type=2, proto=0, laddr=('192.168.0.102', 40921), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP15 TP30 EP30] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=196, family=2, type=2, proto=0, laddr=('192.168.0.102', 42760), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=203, family=2, type=2, proto=0, laddr=('192.168.0.102', 39584), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP8 TP17 EP17] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=196, family=2, type=2, proto=0, laddr=('192.168.0.102', 36225), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=203, family=2, type=2, proto=0, laddr=('192.168.0.102', 37986), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP12 TP25 EP25] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=214, family=2, type=2, proto=0, laddr=('192.168.0.102', 51786), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=222, family=2, type=2, proto=0, laddr=('192.168.0.102', 33999), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP12 TP24 EP24] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=196, family=2, type=2, proto=0, laddr=('192.168.0.102', 59328), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=203, family=2, type=2, proto=0, laddr=('192.168.0.102', 37191), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP9 TP19 EP19] Invalid or no transfer protocol specified, using default protocol.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=213, family=2, type=2, proto=0, laddr=('192.168.0.102', 36279), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=213, family=2, type=2, proto=0, laddr=('192.168.0.102', 46677), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=222, family=2, type=2, proto=0, laddr=('192.168.0.102', 44432), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:2698: ResourceWarning: unclosed <socket.socket fd=222, family=2, type=2, proto=0, laddr=('192.168.0.102', 45546), raddr=('8.8.8.8', 80)>
  if ip := get_local_ip_by_remote():
[2025-12-25 13:13:39 DP9 TP18 EP18] Invalid or no transfer protocol specified, using default protocol.
[2025-12-25 13:13:39 DP13 TP26 EP26] Invalid or no transfer protocol specified, using default protocol.
[2025-12-25 13:13:50] Dummy health check server started in background thread at 192.168.0.102:8000
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
